[{"content":"Let\u0026rsquo;s say you have chosen to use Template Spec for your Bicep code. And you are using these Template specs as templates for your organization in your applications. Now you have a centralized place for your template specs. But you have also introduced a version dependency. Let\u0026rsquo;s explore some challenges with this approach.\nPossible solutions There are some choices that you can make:\nAlways use the latest version of the template spec Use a specific version of the template spec Using Latest version templatespec-latest.bicep 1 2 3 4 5 6 7 8 //https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/modules#path-to-a-module module smartkeysKeyvault \u0026#39;ts/\u0026#39; = { name: \u0026#39;smartkeys\u0026#39; params: { name: \u0026#39;smartkeys\u0026#39; } } Pros You will always build on the latest version of the template specs, this allows you to freely use the latest features and fixes that are available in the template spec. New builds will automatically break when a breaking change is introduced in the template spec, so you have a quick feedback loop.\nCons When a template spec has a breaking change in the latest version, your deployment will break. This can lead to a lot of issues when critical applications are deployed.\nA rollback of an earlier ran pipeline will be insufficient because when deploying bicep using templace spec it will then fetch again the latest version and will result in the same result as the latest deployment. In short, you will have no control.\nUsing a specific version templatespec.bicep 1 2 3 4 5 6 7 8 //https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/modules#path-to-a-module module smartkeysKeyvault \u0026#39;br/public:avm/res/key-vault/vault:0.9.0\u0026#39; = { name: \u0026#39;smartkeys\u0026#39; params: { name: \u0026#39;smartkeys\u0026#39; } } Pros You have full control over what version you are using for deploying your infrastructure. This allows you to test the new version of the template spec before deploying it to production.\nCons Updating versions for many applications can be a lot of work. When you have for example 100 applications that are using the template spec, you have to update all of them when a newer secure version is released.\nThis does happen some examples of this are:\nThe application insights key is deprecated and connection string should be used. The 1.2 TLS was configured and now 1.3 should be the default. These can be requirements by your company and you will have to update to comply to them.\nConsiderations Use a specific version and use a dependency updater What you could do is use a dependency updater like dependabot. This will automatically create a PR when a new version is available. This way you can review the changes. Just as if a colleague would update the template specs.\nAt the moment of writing this blog, bicep and template specs are not supported by dependabot. It is however already available for terraform, so i expect that the need for this will be there in the future. https://docs.github.com/en/code-security/dependabot/dependabot-version-updates/configuration-options-for-the-dependabot.yml-file#package-ecosystem\nSave the latest version build-time Another option is when you build your application to build your bicep code to plain old ARM. This is a JSON-file that will be static when generated, and allows you to use the latest version at build time. You can rerun your old deployments using this static ARM teplate to get an consistent deployment.\nOne note this will not prevent unwanted errors in new builds, your application can still fail when a breaking change is introduced in the template spec.\nConclusion There is not a one size fits all but when you are using template specs in your organization you should think about how you want to handle the versioning of these template specs.\nIt can be a lifesaver to run latest versions when you have good tests in place in your CI/CD pipelines. But realise that you as a developer are taking a risk using this deployment method. When using a specific version you have more control but you will have to update all your applications when a new version is required.\n","permalink":"https://martdegraaf.github.io/posts/bicep-template-spec-latest/","summary":"When using template specs in your organization you should think about how you want to handle the versioning of these template specs. Read it in this blog post.","title":"Bicep Template Spec Latest"},{"content":"GitHub Copilot has made his way into your daily life. For many developers habits are made, but please use the feature that suits your case best. Don\u0026rsquo;t use the Chat or completion as golden hammer for every problem.\nThe goals in this post are to make you aware of the possibilities and to make you think about how you should look at tools such as GitHub Copilot.\nGitHub Copilot GitHub Copilot is super cool. Lets see those features!\nChat The chat feature in VS or VS Code. It\u0026rsquo;s ChatPGT with knowledge of your code. It\u0026rsquo;s handy to get a quick answer to a question. It\u0026rsquo;s not a golden hammer for every problem. What is the difference when u ask it to do something and you are still copy pasting code?\nThe chat window can be a big part of your screen, be aware of that.\nCode completions Use a comment to get a completion. When entering the code will appear. You should know by the way that you can also use only the beginning of the suggestion.\nUse Ctrl + right arrow to accept one word at the time of the completion.\nInline chat For example using Ctrl + i in VS Code. It will give you a completion for the line you are on.\n1 2 3 4 5 6 7 /tests /fix /explain /docs /help /optimize /generate Chat pallete Use Ctrl + Shift + I in VS Code to open the chat in a pallete. This is handy to get a quick chat while keeping focus on your code.\nTo close the chat pallete, use Esc.\nCommit message generation use the Stars to generate a commit message. It\u0026rsquo;s a good way to save time thinking of a commit message. Reference a file/method in a prompt Use the # prefix to reference a file in your prompt.\n1 /fix This method should return a #Address.cs Reference a scope Use the @ prefix to reference a file in your prompt.\nPrompting Your prompt is is key for good results. You should be clear in what you want to achieve.\nIf your first prompt is not giving you the results you want, try to be more specific.\nCode worthy Let me give you a statement:\nCode is less valuable if it\u0026rsquo;s generated by a tool.\nThat is a bold statement. I think it\u0026rsquo;s true. The engineers should be aware of the problem and the solution. The tool can help you with the implementation.\nEngineers are puppeteers The software engineers of the next years are going to tell the story using their puppets. The management will see output of the tooling and think they can do without the engineers. The technology is not there yet, but i think the engineers without the use of AI tooling will be pruned, just like variants in Loki, the Marvel series.\nConclusion and discussion I Like to have focus and not be distracted by the chat window. I think you should use the feature that suits your case best. For generating tests for example i think its best to do it inline, and accept the completion afterwards instead of copy pasting parts of the code.\nI also think you should be reading release notes of GitHub Copilot. It\u0026rsquo;s a tool that is constantly evolving. It\u0026rsquo;s good to know what the changes are.\nReferences https://learn.microsoft.com/en-us/visualstudio/ide/copilot-chat-context?view=vs-2022 ","permalink":"https://martdegraaf.github.io/posts/make-github-copilot-a-habit/","summary":"Now with the capabilities of AI, we code on steroids. Let\u0026rsquo;s make it a habit to use it the right way.","title":"Make Github Copilot a Habit"},{"content":"Visual Studio Code Command Description Ctrl + Shift + P or F1 Open command palette Ctrl + i Open GitHub Copilot inline Ctrl + Shift + i Open GitHub Copilot F2 Rename variables and methods Alt + Shift + F Format document Ctrl + / Comment out code Ctrl + Shift + O Remove unused usings If you want a one-page cheatsheet, check https://code.visualstudio.com/shortcuts/keyboard-shortcuts-windows.pdf.\nVisual Studio Command Description Ctrl + Q Visual Studio search Ctrl + - Ctrl + Shift - Navigating Backward Navigating Forward Alt + / Open GitHub Copilot Ctrl + R, Ctrl + R Rename variables and methods Ctrl + K, Ctrl + D Format document Ctrl + K, Ctrl + C Ctrl + K, Ctrl + U Comment out code Uncomment code Ctrl + R, Ctrl + G Remove and sort usings If you want all from Microsoft, check their cheatsheet for Visual Studio at https://visualstudio.microsoft.com/keyboard-shortcuts.pdf.\nGit Fork I am frequently using Git Fork. Here are some commands I think help me be more productive.\nCommand Description Ctrl + P Show Quick launch window Ctrl + Enter Commit Ctrl + Shift + Enter Commit and push Ctrl + Tab / Shift Tab Next tab / previous tab Ctrl + Shift + B New branch Ctrl + Shift + T New tag Ctrl + Shift + H Create stash Windows I am frequently using Git Fork. Here are some commands I think help me be more productive.\nCommand Description Windows + L Windows lunch, lock your PC when you \u0026rsquo;re away. Windows + Shift + S Screenshot via snipping tool Windows + Shift + R Video snip via snipping tool Windows + Plus (+) Zoom in using magnifier See all the shortcuts at https://github.com/fork-dev/TrackerWin/issues/333.\nGit Fix an casing issue in your files.\n1 2 git mv --force address.cs Address.cs git mv --force \u0026lt;old_file\u0026gt; \u0026lt;new_file\u0026gt; Creating an annotated tag using the -a flag.\n1 git tag -a v1.4 -m \u0026#34;my version 1.4\u0026#34; NPM Update multiple packages at once using the scope.\n1 npx update-by-scope \u0026#34;@angular\u0026#34; ","permalink":"https://martdegraaf.github.io/posts/marts-cheatsheet/","summary":"Get the most out of tooling using Mart\u0026rsquo;s cheatsheet.","title":"Marts Cheatsheet"},{"content":"Imagine you have an application gateway deployed in Azure. Behind this gateway, multiple app services handle requests for different domains. Now, you\u0026rsquo;re adding a new application that needs to be ready for the GO-live moment. We want to deploy an application to production, but we want to restrict access until GO-live. We also want a select number of IP addresses to have access to the new application. Only the company or the development team should have access to the new application at this point.\nIn this blog post, I\u0026rsquo;ll show you how to restrict access to your backend services using a Web Application Firewall (WAF) policy. We\u0026rsquo;ll create a Bicep file, where I\u0026rsquo;ll introduce the first custom type that I have deployed to production!\nExample Azure infrastructure Restricting access When you want to release new code to production but still want it not exposed to the outside world, you can restrict access to the backend services by IP. This can be done by adding custom rules to your web application firewall policy (also known as a WAF-policy).\nAdding custom rules in the portal In the Azure portal, you can add custom rules to your WAF-policy. This can be done by navigating to the WAF-policy and selecting the Custom rules blade. Here you can add a new rule and select the action you want to take when the rule is matched. The portal was easy to uncover all possibilities, every field is a dropdown. This gave me insights into the capabilities of the WAF-policy custom rules. In the portal is visible that multiple conditions in a custom rule are logically using the AND condition. Let\u0026rsquo;s put these insights into repeatable and releasable code!\nCustom policies of a WAF policy in the Azure Portal Defining a custom type Let\u0026rsquo;s use custom types to give ourselves a better intellisense when configuring the parameters.\nWAFPolicyExclusions.bicep 1 2 3 4 5 6 @export() type restrictedDomain = { name: string allowedIPs: array url: string } Parameters To use the custom type we need to define a parameter using the restrictedDomain type. To always allow a set of IP addresses we will add a defaultAllowedIPs array parameter. Those will be unioned with the ones given for a specific domain.\nWAFPolicyExclusions.bicep 11 12 13 14 param defaultAllowedIPs array = [ \u0026#39;127.0.0.1\u0026#39; // Your company IP ] param restrictedDomains restrictedDomain[] = [] Custom rules Within the WAF-policy resource, we can provide custom rules. To do so we use the a for loop. The for loop loops over all restrictedDomains to add a custom rule for each domain.\nThe default action for this rule will be block, we want to block access to a domain except for the allowed IP addresses.\nCondition 1 - Does the domain match For the first condition, we will add a rule that matches the Host in the RequestHeaders, this has to match the given domain. I chose for BeginsWith because I would not want people to make mistakes and possibly take down all domains.\nCondition 2 - Is the IP address in the whitelist? For the second condition, we will add a rule that matches the RemoteAddr, this has to NOT match the given IP addresses. please note the negationConditon: true to make sure the rule is matched when the IP address is not in the list.\nWAFPolicyExclusions.bicep 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 customRules: [ for (item, index) in restrictedDomains: { name: \u0026#39;${item.name}ipBlock\u0026#39; priority: index ruleType: \u0026#39;MatchRule\u0026#39; action: \u0026#39;Block\u0026#39; matchConditions: [ { matchVariables: [ { variableName: \u0026#39;RequestHeaders\u0026#39; selector: \u0026#39;Host\u0026#39; } ] matchValues: [ item.url ] operator: \u0026#39;BeginsWith\u0026#39; transforms: [ \u0026#39;Lowercase\u0026#39; ] negationConditon: false } { matchVariables: [ { variableName: \u0026#39;RemoteAddr\u0026#39; } ] matchValues: union(defaultAllowedIPs, item.allowedIPs) operator: \u0026#39;IPMatch\u0026#39; negationConditon: true } ] } Lessons learned The process of building this custom rule was done in only two days. I learned two things that I want to share with you.\nThe name of the rule has to be unique. The priority must be unique and can only be a value between 1 and 100. This also means you only can have 100 custom rules in a WAF policy. This is a limitation you have to take into account when designing your solution.\nFull Solution Here\u0026rsquo;s a full bicep file with the custom type, parameters, and custom rules.\nWAFPolicyExclusions.bicep 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 @export() type restrictedDomain = { name: string allowedIPs: array url: string } param location string = resourceGroup().location param tags object = resourceGroup().tags param defaultAllowedIPs array = [ \u0026#39;127.0.0.1\u0026#39; // Your company IP ] param restrictedDomains restrictedDomain[] = [] resource WebApplicationFirewallPolicies \u0026#39;Microsoft.Network/ApplicationGatewayWebApplicationFirewallPolicies@2023-11-01\u0026#39; = { name: \u0026#39;WafPolicy\u0026#39; location: location tags: tags properties: { customRules: [ for (item, index) in restrictedDomains: { name: \u0026#39;${item.name}ipBlock\u0026#39; priority: index ruleType: \u0026#39;MatchRule\u0026#39; action: \u0026#39;Block\u0026#39; matchConditions: [ { matchVariables: [ { variableName: \u0026#39;RequestHeaders\u0026#39; selector: \u0026#39;Host\u0026#39; } ] matchValues: [ item.url ] operator: \u0026#39;BeginsWith\u0026#39; transforms: [ \u0026#39;Lowercase\u0026#39; ] negationConditon: false } { matchVariables: [ { variableName: \u0026#39;RemoteAddr\u0026#39; } ] matchValues: union(defaultAllowedIPs, item.allowedIPs) operator: \u0026#39;IPMatch\u0026#39; negationConditon: true } ] } ] managedRules: { managedRuleSets: [ { ruleSetType: \u0026#39;OWASP\u0026#39; ruleSetVersion: \u0026#39;3.0\u0026#39; ruleGroupOverrides: [ { ruleGroupName: \u0026#39;REQUEST-920-PROTOCOL-ENFORCEMENT\u0026#39; rules: [ { ruleId: \u0026#39;920350\u0026#39; action: \u0026#39;Block\u0026#39; } ] } ] } ] } } } And of course an example bicep parameters file.\nWAFPolicyExclusions.bicepparam 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 using \u0026#39;./WAFPolicyExclusions.bicep\u0026#39; param defaultAllowedIPs = [ \u0026#39;127.0.0.1\u0026#39; // Your company IP ] param restrictedDomains = [ { name: \u0026#39;MartService\u0026#39; url: \u0026#39;mart.contoso.com\u0026#39; allowedIPs: [] } { name: \u0026#39;SMartService\u0026#39; url: \u0026#39;smart.contoso.com\u0026#39; allowedIPs: [ \u0026#39;0.0.0.0\u0026#39; // Someone who needs access ] } ] Deploying The only thing to do now is to deploy the bicep files. When you want to enable access to a certain domain you can remove the domain from the parameters file. In this specific case, the application gateway was used for multiple domains and many web applications. Therefore it was easier to restrict access to a specific domain on the WAF-policy. This can be different for your specific use case.\nConclusion and discussion The application gateway can be used to restrict access to your backend services. This can be done by adding custom rules to your WAF-policy. Keep in mind that this has some limits.\nReferences https://learn.microsoft.com/en-us/azure/web-application-firewall/ag/custom-waf-rules-overview https://alanta.nl/posts/2021/04/manage-waf-rules-for-appgateway https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/user-defined-data-types ","permalink":"https://martdegraaf.github.io/posts/apply-ip-restrictions-application-firewall/","summary":"In this blog post, I\u0026rsquo;ll show you how to restrict access by IP addresses and domain names to your backend services using a Web Application Firewall (WAF) policy. We\u0026rsquo;ll create a Bicep file, where I\u0026rsquo;ll introduce the first custom type that I have deployed to production!","title":"Apply Ip Restrictions Application Firewall"},{"content":"Have you ever felt guilty for not being productive? I have. Let\u0026rsquo;s talk about it.\nThe office == work? With more than 10 years of work experience, I have worked in the office, hybrid, and remote. I want to talk about productivity guilt. When I was in the office, I never felt guilty when I was not productive. When I checked in and out of the office those were my working hours. I was there, I was working. There was no guilt, even when drinking coffee with colleagues or smashing a ping pong ball in the office.\nNow with hybrid and remote work, I feel more guilty. I have a family with four children and that takes time, and that does feel like I burn time that was supposed to be for working. As a result, I work later in the evening to compensate for my guilty feelings.\nThe guilt If you do not experience this feeling let me explain what this does for me. This feeling makes me work more hours, and work more in the evening. I feel like I have to make up for the time I was not productive. For example, after a Thursday full of work, the thought comes up to work more to pre-compensate for the Friday after. But on Friday I work the full amount of hours. In the end, I am working more hours than I should.\nImposter syndrome I think this productivity guilt is a form of imposter syndrome. I think this is related because the feeling is also about feeling not good enough. We as programmers often talk about the \u0026lsquo;Imposter syndrome\u0026rsquo;. The feeling that other people know more than you, the imposter syndrome can prevent you from public speaking about your expertise.\nTalking helps I have talked about this with colleagues and they scoped the problem differently. Some quotes came along: \u0026ldquo;Work never ends\u0026rdquo;, and It does not mean the inner voice will stop talking. I think it is important to talk about this and share your feelings with others. It helps to get instructions to stop, or to get compliments on your results.\nDiscipline To get control of your productivity guilt, my approach will be to work on habits. Because i think habits will help you to regulate working hours and feel productive. Work on habits regarding your work. Habits will help you to regulate working hours and feel productive. Some tips I have gathered are:\nStart and end your workday at the same time. Make sure you have a dedicated workspace. Shut down your computer at the end of the workday. In the book \u0026lsquo;Tiny Habits\u0026rsquo; by BJ Fogg, he talks about the MAUI habit. This habit is saying when your feet touch the floor in the morning: \u0026ldquo;It\u0026rsquo;s going to be a great day.\u0026rdquo;. This helps you to start your day with a positive mindset.\nI suggest another habit to enclose your working day. I will try and make this my habit:\nAfter I shut down my computer, I will say to myself: \u0026ldquo;I did a great job today.\u0026rdquo; Final thoughts I am still struggling with this feeling. If this is what people mean by Work-Life balance, I have to learn daily to balance this correctly.\nIf you experience this too, I invite you to share your thoughts with me. I am curious how you cope with this feeling.\nFurther reading While fetching feedback from friends and colleagues, Jan mentioned he fixed his imposter syndrome by using the \u0026lsquo;Circle of Influence\u0026rsquo; by Stephen Covey. He wrote a very simple tool to make you think about a concern. You can find the tool at CircleOfInfluenceTool. The circle of influence is from the book \u0026lsquo;The 7 Habits of Highly Effective People\u0026rsquo; by Stephen Covey. It is a great book to read.\n","permalink":"https://martdegraaf.github.io/posts/productivity-guilt/","summary":"Have you ever felt guilty for not being productive? I have. Let\u0026rsquo;s talk about it.","title":"Productivity Guilt"},{"content":"For a project, we have routes that live in states. We wanted to navigate to a route relative to the current route. for each child route, we will be able to navigate to the previous and next route.\nSystem context Let\u0026rsquo;s say we have a app-routing.module.ts with the following routes:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 const routes: Routes = [ { path: \u0026#39;parent\u0026#39;, component: ParentComponent, children: [ { path: \u0026#39;child1\u0026#39;, component: Child1Component, data: { stepName: \u0026#39;child1\u0026#39;} }, { path: \u0026#39;child2\u0026#39;, component: Child2Component, data: { stepName: \u0026#39;child2\u0026#39;} }, { path: \u0026#39;child3\u0026#39;, component: Child3Component, data: { stepName: \u0026#39;child3\u0026#39;} } ] } ]; Router.navigate We do not want to edit the url property, because it can lead to complexity. You would just say Angular does have the router to do so. The current route is a chain of activated routes when you are navigating to url like /parent/child1/child2/child3.\n1 2 3 4 5 6 7 8 9 10 11 //Inside Some component constructor( private router: Router, private activatedRoute: ActivatedRoute ) {} navigate() { let lastChildRoute = getLastActivatedRouteInChain(this.activatedRoute); this.router.navigate([\u0026#39;../child2\u0026#39;], { relativeTo: lastChildRoute }); } To get the activated route for for example child3 when the url is /parent/child1/child2/child3 we need to get the last child route in the chain. This ActivatedRoute object is passed to the relativeTo property of the router.navigate method.\n1 2 3 4 5 6 7 export const getLastActivatedRouteInChain = (activatedRoute: ActivatedRoute): ActivatedRoute =\u0026gt; { let lastRoute = activatedRoute; while (lastRoute.firstChild) { lastRoute = lastRoute.firstChild; } return lastRoute; }; Conclusion and discussion This way we can navigate to a route relative to the current route. This is useful when you have a wizard-like flow in your application.\nFurther reading Angular Router ","permalink":"https://martdegraaf.github.io/posts/angular-relative-navigation/","summary":"Learn how to navigate to a route relative to the current route in Angular. This is useful when you have a wizard-like flow in your application.","title":"Angular Relative Navigation"},{"content":"In a previous blog post, we tackled the issue of pushing files and creating pull requests using Powershell. In this blog post, we are going to tackle the issue of migrating classic releases to YAML pipelines in Azure DevOps. We are going to use a script to fetch current classic releases and make yaml files out of it.\n\u0026#x1f916; Check out my previous blog to get more context: Foreach repositories push and create a pull request. One script to rule them all This thing is not a one-size-fits-all solution. This script fetches all variables and puts them in variable files for you and puts a yaml file that extends an existing template. The template is let out of scope. The script is a starting point for you to migrate your classic releases to YAML pipelines and might give you ideas and inspiration.\nWhy automate Automation has benefits over manual work when exceeding a number of repositories. As visualized in the chart above we can easily calculate the time saved by automating the process. When the time of the automation is smaller than the number of repositories times the time to do it manually, we should automate. In this case, we have 60 repositories and the time to do it manually is 1 hour per repository. I set the time to automate to 32 hours. This is the time I spent on the script. The time to do it manually would be 60 hours. So we saved 28 hours by automating the process. And we can reuse the script for future migrations. Or now you can too \u0026#x1f609;.\nExport release and archive it In this script, a release is fetched from Azure DevOps using the Azure DevOps REST API. The release is then exported to a yaml file and moved to an archive folder. We can use the output of this script as input for our script to push files and create a pull request.\nReleaseExportAndMoveToArchive.ps1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 # Description: This script moves a release definition to a target folder in Azure DevOps # it depends on you being signed in in the Azure cli, that can be done by `az login` # Usage: ReleaseExportAndMoveToArchive.ps1 -releaseDefinitionName \u0026#34;MartService Release\u0026#34; -serviceName \u0026#34;MartService\u0026#34; param ( [Parameter(Mandatory=$true)] [string]$releaseDefinitionName, [Parameter(Mandatory=$true)] [string]$serviceName ) # Prompt the user to login and get the access token # see https://learn.microsoft.com/en-us/azure/devops/integrate/get-started/authentication/service-principal-managed-identity?toc=%2Fazure%2Fdevops%2Forganizations%2Fsecurity%2Ftoc.json\u0026amp;view=azure-devops#q-can-i-use-a-service-principal-or-managed-identity-with-azure-cli $accessToken = az account get-access-token --resource 499b84ac-1321-427f-aa17-267ca6975798 --query \u0026#34;accessToken\u0026#34; --output tsv if ($accessToken -eq $null) { exit 1 } $orgUrl = \u0026#34;https://vsrm.dev.azure.com/MartOrg\u0026#34; $project = \u0026#34;MartProject\u0026#34; $targetFolderId = \u0026#34;/Archive\u0026#34; $headers = @{ \u0026#34;Authorization\u0026#34; = (\u0026#34;Bearer {0}\u0026#34; -f $accessToken) \u0026#34;Accept\u0026#34; = \u0026#34;application/json\u0026#34; } # Get the release definitions matching the given name $uri = \u0026#34;$orgUrl/$project/_apis/release/definitions?api-version=7.0\u0026#34; $definitionsResponse = Invoke-RestMethod -Uri $uri -Headers $headers -Method Get -ContentType \u0026#34;application/json\u0026#34; Write-Host \u0026#34;Found $($definitionsResponse.count) release definitions\u0026#34; $definition = $definitionsResponse.value | Where-Object { $_.name -eq $releaseDefinitionName } # Move the release definition to the target folder $definition.id = $definition.id -replace \u0026#34;:\u0026#34;, \u0026#34;%3A\u0026#34; # Escape the colon in the definition ID $uri = \u0026#34;$($definition.url)?api-version=7.0\u0026#34; Write-Host \u0026#34;$uri\u0026#34; # get the current release definition $pipeline = Invoke-RestMethod -Uri $uri -Headers $headers -Method Get -ContentType \u0026#34;application/json\u0026#34; $pipeline.path = \u0026#34;$targetFolderId\u0026#34; $json = @($pipeline) | ConvertTo-Json -Depth 99 # create serviceName folder New-Item -ItemType Directory -Force -Path \u0026#34;tmp\\$($serviceName.ToLower())\u0026#34; Function ConvertTo-Yml { param ( [Parameter(Mandatory=$true)] [object]$object, [Parameter(Mandatory=$true)] [string]$Path ) $yml = \u0026#39;variables:\u0026#39; foreach ($variable in $object.PsObject.Properties) { $yml += \u0026#34;`n $($variable.Name): $($variable.Value.Value)\u0026#34; } Write-Host $yml $yml | Out-File -FilePath $Path } ConvertTo-Yml $pipeline.variables \u0026#34;tmp\\$($serviceName.ToLower())\\variables.yml\u0026#34; foreach ($environment in $pipeline.environments) { #transform $environment.name to o, t, a or p #Possible inputs: Development, Test, Acceptance, Production $environmentName = $environment.name.ToLower().Substring(0,1) $environmentSuffix = switch ($environmentName) { \u0026#34;d\u0026#34; { \u0026#34;o\u0026#34; } \u0026#34;t\u0026#34; { \u0026#34;t\u0026#34; } \u0026#34;a\u0026#34; { \u0026#34;a\u0026#34; } \u0026#34;p\u0026#34; { \u0026#34;p\u0026#34; } } ConvertTo-Yml $environment.variables \u0026#34;tmp\\$($serviceName.ToLower())\\variables-$($environmentSuffix).yml\u0026#34; } $yml = \u0026#34; trigger: branches: include: - main pool: \u0026#39;default\u0026#39; resources: repositories: - repository: Pipelines type: git name: Pipelines ref: refs/heads/main name: `$(Build.DefinitionName)_`$(SourceBranchName)_`$(date:yyyyMd).`$(Rev:r) variables: - template: variables.yml extends: template: Yml/service.pipeline.yml@Pipelines parameters:\u0026#34; $yml | Out-File -FilePath \u0026#34;tmp\\$($serviceName.ToLower())\\$($serviceName.ToLower())-pipeline.yml\u0026#34; # update the release definition with the new path $response = Invoke-RestMethod -Uri $uri -Headers $headers -Method Put -Body $json -ContentType \u0026#34;application/json\u0026#34; Write-Host \u0026#34;Release definition \u0026#39;$releaseDefinitionName\u0026#39; moved to folder \u0026#39;$targetFolderId\u0026#39;\u0026#34; Conclusion We have now migrated our classic releases to YAML pipelines. We can now use the output of this script to push files and create a pull request. This way we can automate the process of updating all our repositories with the same type of change. We have saved time and can now focus on browsing/chilling/netflixing other tasks.\n","permalink":"https://martdegraaf.github.io/posts/consulting/migrating-classic-to-yaml-releases/","summary":"Migrate your classic releases to YAML pipelines in Azure DevOps with our comprehensive guide. Streamline your deployment process today!","title":"Migrating Classic Azure DevOps Releases to YAML Pipelines"},{"content":"For a client, we manage over 60 repositories with frontends and an API behind them. To realize some features, we need to update all repositories with the same change. That seems like a lot of work doesn\u0026rsquo;t it?\nClone 60 projects? I talked in previous posts about automating these kinds of changes. But for this case, we don\u0026rsquo;t want to clone all 60 repositories. We could go for a temp folder and clone them one by one and then clean them up. Another solution could be creating a yaml pipeline for it, that clones on the agent machine and loops over all repositories, but I wanted to keep the blame on the person of the initial change.\nFor this case I used GitHub copilot to help me brainstorm for solutions, another solution I did not follow was using a $gitClient object in Powershell.\nPush files In the underlying script, I can create a branch from the main branch and push the contents of the readme file. This is a simple example, but it could be any file. The script can push the file to the repository and create a pull request.\nPushReadmeAzureDevOps.ps1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 # Prompt the user to login and get the access token # see https://learn.microsoft.com/en-us/azure/devops/integrate/get-started/authentication/service-principal-managed-identity?toc=%2Fazure%2Fdevops%2Forganizations%2Fsecurity%2Ftoc.json\u0026amp;view=azure-devops#q-can-i-use-a-service-principal-or-managed-identity-with-azure-cli $accessToken = az account get-access-token --resource 499b84ac-1321-427f-aa17-267ca6975798 --query \u0026#34;accessToken\u0026#34; --output tsv if ($null -eq $accessToken) { exit 1 } $orgUrl = \u0026#34;https://dev.azure.com/MARTORG\u0026#34; $project = \u0026#34;MARTPROJECT\u0026#34; $repositoryId = \u0026#34;YOURREPO\u0026#34; $readmeFilePath = \u0026#34;README.md\u0026#34; $headers = @{ \u0026#34;Authorization\u0026#34; = (\u0026#34;Bearer {0}\u0026#34; -f $accessToken) \u0026#34;Accept\u0026#34; = \u0026#34;application/json\u0026#34; \u0026#34;Content-Type\u0026#34; = \u0026#34;application/json\u0026#34; } # Convert the README file to base64 $readmeContent = Get-Content -Path $readmeFilePath -Raw $readmeBase64 = [System.Convert]::ToBase64String([System.Text.Encoding]::UTF8.GetBytes($readmeContent)) Write-Host \u0026#34;README file content: $readmeContent\u0026#34; # get current commit object id $commitUrl = \u0026#34;$orgUrl/$project/_apis/git/repositories/$repositoryId/refs/heads/main?api-version=6.0\u0026#34; $commitResponse = Invoke-RestMethod -Uri $commitUrl -Headers $headers -Method Get Write-Host $commitResponse.Value $commitObjectId = $commitResponse.Value.objectId Write-Host \u0026#34;Current commit object id: $commitObjectId\u0026#34; # Create the request body $changes = @( @{ changeType = \u0026#34;edit\u0026#34; item = @{ path = \u0026#34;README.md\u0026#34; } newContent = @{ content = $readmeBase64 contentType = \u0026#34;base64encoded\u0026#34; } } ) $requestBody = @{ refUpdates = @( @{ name = \u0026#34;refs/heads/add-readme\u0026#34; oldObjectId = $commitObjectId } ) commits = @( @{ comment = \u0026#34;Adding README file\u0026#34; changes = $changes } ) } | ConvertTo-Json -Depth 6 # Set the API endpoint $apiUrl = \u0026#34;$orgUrl/$project/_apis/git/repositories/$repositoryId/pushes?api-version=6.0\u0026#34; # Send the API request $response = Invoke-RestMethod -Uri $apiUrl -Method Post -Headers $headers -Body $requestBody # Check the response if ($response.pushId) { Write-Host \u0026#34;README file pushed successfully.\u0026#34; } else { Write-Host \u0026#34;Failed to push README file. Error: $($response.message)\u0026#34; } # create a pull request for myfirstbranch $pullRequestUrl = \u0026#34;$orgUrl/$project/_apis/git/repositories/$repositoryId/pullrequests?api-version=6.0\u0026#34; $pullRequestBody = @{ sourceRefName = \u0026#34;refs/heads/add-readme\u0026#34; targetRefName = \u0026#34;refs/heads/main\u0026#34; title = \u0026#34;Add README file\u0026#34; description = \u0026#34;Adding README file\u0026#34; } | ConvertTo-Json -Depth 6 $pullResponse = Invoke-RestMethod -Uri $pullRequestUrl -Method Post -Headers $headers -Body $pullRequestBody if ($pullResponse.pullRequestId) { Write-Host \u0026#34;Pull request created successfully.\u0026#34; } else { Write-Host \u0026#34;Failed to create pull request. Error: $($pullResponse.message)\u0026#34; } Conclusion I am going to use this script to update files across all of our repositories. For this client, the code review is important, so the automation would end in a set pull request. In the next blog post, I am going to create a script that will create files to be pushed for every repository.\n","permalink":"https://martdegraaf.github.io/posts/consulting/foreach-repo-push-and-pr/","summary":"Automate repetitive tasks using code, even when it is in code.","title":"Foreach repository push files and create a pull request"},{"content":"Introduction Artificial intelligence is hip and happening. Everyone can use ChatGPT right now easily. But there are some worries about where your chats will go. Is it safe? Is the model trained by my conversations?\nAs a consultant, it is important to know the possibilities and the tradeoffs of using AI. In this article, we discuss the OpenAI services in Azure. The Azure OpenAI services promise that the data stays in your environment in the cloud. This is a minimal requirement for many companies. We will show how to use the playground, the API, and the Azure Cognitive Search integration.\nCreating your OpenAI resource Before you can start using Azure OpenAI you will need to be accepted by Microsoft. You need to apply for the \u0026lsquo;Azure OpenAI Service\u0026rsquo; preview. You can do this by following the procedure to create this resource, a link to the application form is provided.\nCreate a new resource in Azure. Search for OpenAI Select the OpenAI resource Create the resource Azure Marketplace - OpenAI Using OpenAI studio After creating the resource you can go to the OpenAI studio.\nOpenAI Studio This is a web-based tool the \u0026lsquo;OpenAI Playground\u0026rsquo; that allows you to test the models and see the results. You can also create your models and train them.\nCreate a new Model You can create a new model by clicking the \u0026lsquo;Create Model\u0026rsquo; button. You can select the model you want to use. The default is the GPT-3 turbo model. I did my testing with the GPT-35 turbo Model. From the model, you need to make a deployment. This will take a few minutes. After the deployment is done you can start using the model. The DaVinci model can be a better fit for autocompletion and text generation. Dall-E is only available in East US.\nChat Playground In the chat playground, there are some cool features:\nAdd data sources Add system prompts Clear the chat, and view the code Parameters into the model. Import and export the setup. The Chat Playground doesn\u0026rsquo;t have a dark mode, so beware of your eyes if you do this at nighttimes.\nOpenAI Studio - Chat Playground Defining System Prompts The default will get you somewhere but you can also use the system prompts to get better results. The system prompts are a way to give the model some context. For example, if you want to create a chatbot you can use the following system prompt:\n1 2 3 4 You are an AI assistant named Mart that helps people find information. The assistant is helpful, creative, clever, and very friendly. Provide the full schedule when the user asks for a certain date. End every message with \u0026#34;Mart knows everything\u0026#34;. Add a mermaid diagram written in markdown to clarify your statement. This kind of prompting will result in some hilarious results. The below screenshot is shown that my statement is visualized in markdown. GPT drew me a mermaid :).\nSample Chat After correcting the prompt to Add a mermaid diagram written in markdown to clarify your answer on the question.. I got the response:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Azure is a cloud computing platform that offers a wide range of resources to help users build, deploy, and manage applications and services. Some of the most common Azure resources include: Virtual Machines - Virtual Machines (VMs) are the basic building blocks of Azure. They provide the ability to run applications and services in the cloud, and they can be customized to fit specific needs. Storage Accounts - Storage Accounts are used to store data in the cloud. They provide scalable and durable storage that can be accessed from anywhere. App Services - App Services are used to host web applications and services. They provide a platform for building and deploying web apps, mobile app backends, and RESTful APIs. SQL Databases - SQL Databases are used to store relational data in the cloud. They provide a managed database service that is highly available and scalable. Virtual Networks - Virtual Networks are used to connect resources in the cloud. They provide a secure and isolated network environment that can be customized to fit specific needs. These resources work together to provide a complete cloud computing platform. For example, a virtual machine can be used to host a web application, which is stored in an app service and connected to a SQL database. Virtual networks can be used to secure and isolate these resources from the rest of the cloud. Here\u0026#39;s a mermaid diagram to help clarify: graph LR A[Virtual Machines] --\u0026gt; B[Storage Accounts] A --\u0026gt; C[App Services] C --\u0026gt; D[SQL Databases] A --\u0026gt; E[Virtual Networks] Mart knows everything. When putting it in a mermaid renderer I got the following result:\ngraph LR A[Virtual Machines] --\u003e B[Storage Accounts] A --\u003e C[App Services] C --\u003e D[SQL Databases] A --\u003e E[Virtual Networks] This was more what I expected. Only the mermaid diagram connections don\u0026rsquo;t make sense. This is a limitation, it can\u0026rsquo;t model the connections well enough. You should check the output and test when integrating these features. ChatGPT is a statistical model, so it will give you a result that is the best fit based on its data. That does not imply everything is correct. You should always be critical at the output.\nProperties In the parameters tab, you can play with some settings. You will need to know about those to get the best results.\nMax response The Max response setting sets a limit on tokens that can be used per response. I found better results with complex prompts with the Max response just above 2000.\nTemperature The temperature is a setting that controls the randomness of the model. The higher the temperature the more random the results will be. For more theoretical information see: https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/.\nThe Top P setting works like the temperature setting, 0 is low probability and 1 is high probability. The higher the value, the more diverse probabilities will be allowed.\nDocumenting code to using OpenAI 1 2 3 4 5 6 7 8 9 10 11 12 13 14 create a mermaid diagram for the flow that is done in this c# code public static voidFibonacci_Iterative(int len) { int a = 0, b = 1, c = 0; Console.Write(\u0026#34;{0} {1}\u0026#34;, a,b); for (int i = 2; i \u0026lt; len; i++) { c= a + b; Console.Write(\u0026#34; {0}\u0026#34;, c); a= b; b= c; } } This prompt resulted in the code being explained in a good flowchart. This is a great way to document your code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 To create a mermaid diagram for the flow in the given C# code, you can follow these steps: Start by creating a sequenceDiagram block in your mermaid code. Add a participant for the Fibonacci_Iterative method. Add a participant for the Console class. Add a note to the Fibonacci_Iterative participant to indicate that it takes an integer parameter len. Add a note to the Console participant to indicate that it writes to the console. Add a loop block to represent the for loop in the code. Inside the loop block, add a note to indicate that c is the sum of a and b. Inside the loop block, add a note to indicate that a is assigned the value of b. Inside the loop block, add a note to indicate that b is assigned the value of c. Connect the Fibonacci_Iterative participant to the Console participant with an arrow to indicate that it writes to the console. Here\u0026#39;s an example of what the mermaid code could look like: sequenceDiagram participant Fibonacci_Iterative participant Console Note over Fibonacci_Iterative: Takes integer parameter len Note over Console: Writes to console loop for i=2 to len Note over Fibonacci_Iterative: c = a + b Note over Fibonacci_Iterative: a = b Note over Fibonacci_Iterative: b = c end Fibonacci_Iterative-\u003e\u003eConsole: Writes to console Adding your data using Cognitive Search You can also add your data to the model. This can be done by using the Cognitive Search feature. You can upload your data and use it to train the model. You can also use the data to create completions.\nIn the playground click \u0026lsquo;add your data\u0026rsquo; and click \u0026lsquo;+ add Data source\u0026rsquo;.\nChat playground - Add Data Source Structure Whatever you choose in the data source dropdown you will always end up with a Blob storage containing some blob files with a Cognitive Search resource. Make sure when creating the cognitive search resource you select the same region as the OpenAI resource and it has to be at least basic.\nflowchart LR OpenAI --\u003e |uses| CognitiveSearch CognitiveSearch --\u003e| indexes| BlobStorage Steps to add data Create a Blob storage account and put files in a container. I recommend using the Azure Storage Explorer. Upload some PDF files you want to be able to search for. Or use HTML pages from your public website as feed for your data source. Create a Cognitive Search resource. This has to be at least Basic, performance will be better on higher sku, but costs will be higher too. Create a data source in the Playground. Select \u0026lsquo;Azure Blob Storage\u0026rsquo;. Select the Blob storage resource you created. Select the container you created or want to use. Select the Cognitive Search resource you created. Enter an index name Check the box that you know about the pricing. And continue. Click Save and Close on the Review and Finish page. Wait for the Indexer to finish. You can watch the progress in the Cognitive Search resource. Navigate to the Indexers and Indexes tab. Indexers run to create the index. Cognitive Search - Indexers Chatting to your documents Now that the indexer is ready and the index is loaded, let\u0026rsquo;s chat to documents. The blob storage is fed with a slide deck of my Git presentation as PDF to the blob storage. Let\u0026rsquo;s ask GPT about Git.\nChatting with presentations Please note that in the screenshot you can choose to limit responses specific to your data content. Sources will be shown in the response. This is a great way to get insights into your data.\nCost management The basic sku on Cognitive search is a pay-per-month model, which means you should delete the resource after you are done testing with it. The OpenAI resource is pay-per-use as is the storage account.\nDeploy it to an App Service So you have now seen some power or some stupidity of the OpenAI model. You can also deploy it to an App Service. This can be on a free sku, but make sure when using this that your Chat application will be open on {your-app-name}.azurewebsites.net. You will need to configure VNET integration if you are dealing with company information.\nResource group overview As wrap up for all the resources we created during our testing. We can see the following resources in our resource group. I know you will be all testing if MartGPT is still online.\nResource group overview Using OpenAI API services through C sharp The OpenAI API is a REST API that allows you to use the models in your applications. You can use the API to create completions and chat. Some real power is when we can use the OpenAI services through our code, where we can create our business scope.\nSample case Make sure you have Include prerelease enabled when searching for the NuGet package Azure.AI.OpenAI.\nProgram.cs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 // Note: The Azure OpenAI client library for .NET is in preview. // Install the .NET library via NuGet: dotnet add package Azure.AI.OpenAI --version 1.0.0-beta.5 using Azure; using Azure.AI.OpenAI; var dieren = new[] { \u0026#34;Penguins\u0026#34;, \u0026#34;Giraffes\u0026#34;, \u0026#34;Smurfen\u0026#34;, // more }; OpenAIClient client = new OpenAIClient( new Uri(\u0026#34;https://martgpt-openai.openai.azure.com/\u0026#34;), new AzureKeyCredential(\u0026#34;HERE COMES YOUR API KEY\u0026#34;)); // ### If streaming is not selected foreach (var dier in dieren) { Response\u0026lt;ChatCompletions\u0026gt; responseWithoutStream = await client.GetChatCompletionsAsync( \u0026#34;MartGPT\u0026#34;, //\u0026lt;= This is the deployment name new ChatCompletionsOptions() { Messages = { new ChatMessage(ChatRole.User, dier), new ChatMessage(ChatRole.User, @\u0026#34;Geef antwoord in JSON met categorie property van het type string met als waarde een van deze categorien: Landdieren Zeedieren In welke categorie valt deze diersoort en geef een omschrijving met maximaal 5 woorden en geef ook de naam van het dier terug.\u0026#34;), }, Temperature = (float)1, MaxTokens = 800, NucleusSamplingFactor = (float)1, FrequencyPenalty = 0, PresencePenalty = 0, }); ChatCompletions completions = responseWithoutStream.Value; foreach (var choice in completions.Choices) { Console.WriteLine(choice.Message.Content); } } The output I got from this. I laughed a lot about the response I hope you can do too. What is shown is that even Smurfen is placed in the Landdieren category. Even when you don\u0026rsquo;t edit the parameters or temperature it will give you a different response every time. You have to be very precise if you want to standardize the output.\nProgramOutput.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;categorie\u0026#34;: \u0026#34;Zeedieren\u0026#34;, \u0026#34;omschrijving\u0026#34;: \u0026#34;Vogels die niet vliegen\u0026#34;, \u0026#34;naam\u0026#34;: \u0026#34;Pinguïns\u0026#34; } { \u0026#34;naam\u0026#34;: \u0026#34;Giraffe\u0026#34;, \u0026#34;categorie\u0026#34;: \u0026#34;Landdieren\u0026#34;, \u0026#34;omschrijving\u0026#34;: \u0026#34;Hoge nek, gevlekte vacht\u0026#34; } { \u0026#34;categorie\u0026#34;: \u0026#34;Landdieren\u0026#34;, \u0026#34;omschrijving\u0026#34;: \u0026#34;Blauwe wezentjes in het bos\u0026#34;, \u0026#34;naam\u0026#34;: \u0026#34;Smurfen\u0026#34; } Think of the possibilities by lowering the temperature and making materialized categories of some huge text inputs, for example in an Azure Function reacting to business events.\nConclusion I just wanted to try out the OpenAI API and see the possibilities it has. Soon corporates will be ready to use AI for their use cases and as a consultant, you should be able to advise them on the possibilities. I learned that the GPT model is statistically choosing output, and the response needs to be verified. That\u0026rsquo;s why Microsoft calls its products `Copilot``. It helps you but isn\u0026rsquo;t perfect.\nMay the AI be with you.\nFurther reading https://learn.microsoft.com/en-us/legal/cognitive-services/openai/overview\nhttps://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/completions#completion\nhttps://blog.iusmentis.com/2023/03/21/van-wie-is-mijn-werk-als-ik-chatgpt-mijn-werk-laat-doen/\nhttps://blog.iusmentis.com/2023/06/08/mag-een-iso27001-gecertificeerde-organisatie-chatgpt-gebruiken/\nhttps://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/\n","permalink":"https://martdegraaf.github.io/posts/getting-started-with-azure-openai/","summary":"AI is happening. How can you make sure your organization benefits from AI?","title":"Getting started with Azure OpenAI"},{"content":"Introduction Cloud costs can be real money wasters, if you log something you will never need, who is responsible for that? While looking at your costs in Azure, you could see Application Insights as a big cost driver. In this blog post, I will show you how to get in control of your Application Insights costs.\nPrior knowledge\nIn an earlier post, I showed how to fix duplicate logging and explained how Log Analytics sits on top of Application Insights.\nRead the blog post on How to fix duplicate logging in Application Insights.\nAll KQL queries in this post are based on the Log Analytics workspace.\nIdentify the biggest cost tables Log Analytics consists of tables, each of those with a specific target to log something. For example, AppTraces for traces, but also tables like SynapseIntegrationTriggerRuns for logging Synapse triggers.\nThe first step is to identify the biggest cost tables. You can do this by running the following query in your \u0026lsquo;Log Analytics Workspace\u0026rsquo; resource. The payment model is per Gb, so we want to identify the largest tables.\nLog Analytics Workspace - Logs - Kusto Query Language With knowledge about the biggest cost tables, you can start optimizing your logging. In the next sections, I will show you example queries to give insights into logging costs.\nQueryByTable.kusto 1 2 3 4 5 union withsource = table * | summarize Size = sum(_BilledSize) by table, _IsBillable | sort by Size desc | extend Size2 = format_bytes(Size, 2) | order by Size desc Azure Diagnostic Logs On many Azure resources, you can configure Log Analytics Workspace as an upstream source. But did you know that this can lead to many logs you have to pay for? A colleague of mine used this query to identify and reduce 90% of their costs. By disabling the Azure Diagnostic Logs for Power BI, they saved a lot of money. By running this query you will gain insights into the amount of logs ingested per resource.\nQueryTableByResourceId.kusto 1 2 3 4 AzureDiagnostics | where TimeGenerated \u0026gt; ago(32d) | summarize sum (1) by bin(TimeGenerated, 1d), _ResourceId | render columnchart Application traces Traces are good for hunting bugs. But when a system is running, do you need all Debug logs? Do you even think every log is important?\nIn this query below I will sort unique logging metrics by Resource and Costs. The most expensive logs are on top. The magic number 2,52 was the price per Gb ingested for Log Analytics. When you insert more than 100Gb which is a lot, you can get discounted pricing. Make sure when you query you think of your scope and environments that also log this trace.\nMake sure you configure your log levels correctly. In appsettings.json of host.json.\nAppTracesByCosts.kusto 1 2 3 4 5 6 7 8 9 10 11 12 13 AppTraces | extend MessageSize = strlen(Message) | order by MessageSize desc | summarize Count=count(), BilledTotalSize = sum(_BilledSize), MessageTotalSize = sum(MessageSize) by AppRoleName, OperationName, MessageSize, Message, SeverityLevel, _ResourceId | extend GbSize = BilledTotalSize / 1024 /1024 / 1024 | extend EuroCost = GbSize * 2,52 | extend ResourceName = tostring(split(_ResourceId, \u0026#34;/\u0026#34;)[-1]) | project ResourceName, EuroCost, Count, SeverityLevel, Message | order by EuroCost desc Application dependencies Dependencies are really important. But when writing too much dependency logging it can lead to unwanted costs. This query will give you insights into the dependencies that have a great economic footprint in your Log Analytics Workspace. The EuroCost is determined by the sum of _BilledSize size of all dependencies given in Gb, multiplied by 2,52.\nThe DataTotalSize field indicates the data size, this can contain for example the Database query when that is enabled in your logging. If this value is big and the count of this dependency is high this might be a hotspot to act on.\nAppDependenciesByCosts.kusto 1 2 3 4 5 6 7 8 9 10 11 12 13 AppDependencies | extend DataSize = strlen(Data) | order by DataSize desc | summarize Count=count(), BilledTotalSize = sum(_BilledSize), DataTotalSize = sum(DataSize) by AppRoleName, OperationName, DataSize, Data, _ResourceId, Type | extend GbSize = BilledTotalSize / 1024 / 1024 / 1024 | extend EuroCost = GbSize * 2, 52 | extend ResourceName = tostring(split(_ResourceId, \u0026#34;/\u0026#34;)[-1]) | project ResourceName, OperationName, EuroCost, Count, Data, Type | order by EuroCost desc Health checks A special mention is for health checks, do you need the full trace and dependency tree for every health check call? Make sure to exclude those unwanted requests and dependencies. You might only consider keeping health check logging when the health check fails and only the health check result.\nDashboard By putting the data in a dashboard you will provide your team with an easy way to access these metrics. In my screenshot below there are two of the most important queries, the application traces and the tables.\nMake sure to set your dashboard time to a good time scope.\nTracing costs dashboard Conclusion When turning on diagnostics make sure it helps the business. Revisit diagnostic settings and make sure you are in control of your costs. Also make sure that when in development, you are critical about the diagnostic settings. When turned on, it won\u0026rsquo;t be turned off soon, because now you\u0026rsquo;re the expert!\nFurther reading For making a custom Processor in Code to make conditional logging make sure to visit my mate\u0026rsquo;s blog Thomas Vieveen.\nhttps://azure.microsoft.com/nl-nl/pricing/details/monitor/ ","permalink":"https://martdegraaf.github.io/posts/azure-application-insights-in-control-of-costs/","summary":"Many companies have a huge money bill for application logging. In this blog post, I will show you how to get in control of your Application Insights costs.","title":"Learn how to verify the biggest costs of your Log Analytics workspace"},{"content":"As a consultant when a job ends you need to transition from the previous to the next. How do you efficiently clean up all open ends? The open branches that you leave behind are sort of dead code. Nobody is going to take care of it. Make sure you don\u0026rsquo;t generate more work for ex-colleagues following these tips.\nUsing the scripts below we can create a small to-do list, which you should do before leaving a company.\n💬 \u0026ldquo;The first impression is just as important as the last impression. Make sure you leave a good impression.\u0026rdquo; - Mart de Graaf 1. No open work items Even if you would work with Trello, it would be nice to hand over open items to coworkers. When working in Azure DevOps, you can use the following Powershell script to get all open work items assigned to you.\nIn the PowerShell script below we can easily get all open work items. This is not only handy when leaving companies but also when you want to get an overview of all open work items. You can use this script to get knowledge of all open work items or to hand over the work to a colleague. You could also do this in Azure DevOps, but when you already have a PAT-token, you can check this as well with this easy script.\nOpenWorkItems.ps1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # Set variables $organizationUrl = \u0026#34;https://dev.azure.com/MART\u0026#34; # Replace Mart with organization name $projectName = \u0026#34;ProjectName\u0026#34; # Replace ProjectName with project name # Get the access token from current az login session # see https://learn.microsoft.com/en-us/azure/devops/integrate/get-started/authentication/service-principal-managed-identity?toc=%2Fazure%2Fdevops%2Forganizations%2Fsecurity%2Ftoc.json\u0026amp;view=azure-devops#q-can-i-use-a-service-principal-or-managed-identity-with-azure-cli $accessToken = az account get-access-token --resource 499b84ac-1321-427f-aa17-267ca6975798 --query \u0026#34;accessToken\u0026#34; --output tsv if ($null -eq $accessToken) { exit 1 } # Set headers $headers = @{ \u0026#34;Authorization\u0026#34; = (\u0026#34;Bearer {0}\u0026#34; -f $accessToken) \u0026#34;Accept\u0026#34; = \u0026#34;application/json\u0026#34; } # Get a list of workitems for given username $workItemsUrl = \u0026#34;$organizationUrl/$projectName/_apis/wit/wiql?api-version=6.0\u0026#34; $wiql = @\u0026#34; SELECT [System.Id], [System.Title], [System.State], [System.AssignedTo], [System.Tags], [System.WorkItemType] FROM workitems WHERE [System.TeamProject] = @project AND [System.WorkItemType] = \u0026#39;Task\u0026#39; AND [System.State] \u0026lt;\u0026gt; \u0026#39;Closed\u0026#39; AND [System.State] \u0026lt;\u0026gt; \u0026#39;Removed\u0026#39; AND [System.State] \u0026lt;\u0026gt; \u0026#39;Done\u0026#39; AND [System.AssignedTo] = @me ORDER BY [System.ChangedDate] desc \u0026#34;@ $body = @{ query = $wiql } | ConvertTo-Json $workItemsResponse = Invoke-RestMethod -Uri $workItemsUrl -Headers $headers -Method Post -Body $body -ContentType \u0026#34;application/json\u0026#34; # use workitemsbatch api to get all SELECT values $workItemsUrl = \u0026#34;$organizationUrl/$projectName/_apis/wit/workitemsbatch?api-version=6.0\u0026#34; # get the ids from $workItemsResponse.workItems in a list max 200 $ids = $workItemsResponse.workItems.id | Select-Object -First 200 # body is the list of ids in the workitemsresponse workitems.id, and the fields to select in a fields array $body = @{ ids = $ids; fields = \u0026#34;System.Id\u0026#34;, \u0026#34;System.Title\u0026#34;, \u0026#34;System.State\u0026#34;, \u0026#34;System.AssignedTo\u0026#34;, \u0026#34;System.Tags\u0026#34;, \u0026#34;System.WorkItemType\u0026#34; } | ConvertTo-Json $workItemsResponse2 = Invoke-RestMethod -Uri $workItemsUrl -Headers $headers -Method Post -Body $body -ContentType \u0026#34;application/json\u0026#34; Write-Host \u0026#34;Workitems found \u0026#39;$($workItemsResponse.workItems.count)\u0026#39;\u0026#34; # write the response to a JSON file $workItemsResponse2 | ConvertTo-Json -Depth 100 | Out-File -FilePath \u0026#34;workitems.json\u0026#34; -Force Output It will be output in a JSON file, for now. I think that will be easy to read if you are leaving. It also limits to 200 work items. If you have more than 200 work items, you probably have a problem.\n2. No open branches \u0026#x1f966; or pull requests When working with Git, you can use the following Powershell script to get all open branches. You can use this script to get knowledge of all open branches or to create a pull request for each branch. The pull request can be used to hand over the work to a colleague.\nOpenBranches.ps1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 # Set variables $organizationUrl = \u0026#34;https://dev.azure.com/MART\u0026#34; # Replace Mart with organization name $projectName = \u0026#34;ProjectName\u0026#34; # Replace ProjectName with project name $dayTolerance = 14 # Get the access token from current az login session # see https://learn.microsoft.com/en-us/azure/devops/integrate/get-started/authentication/service-principal-managed-identity?toc=%2Fazure%2Fdevops%2Forganizations%2Fsecurity%2Ftoc.json\u0026amp;view=azure-devops#q-can-i-use-a-service-principal-or-managed-identity-with-azure-cli $accessToken = az account get-access-token --resource 499b84ac-1321-427f-aa17-267ca6975798 --query \u0026#34;accessToken\u0026#34; --output tsv if ($null -eq $accessToken) { exit 1 } # Set headers $headers = @{ \u0026#34;Authorization\u0026#34; = (\u0026#34;Bearer {0}\u0026#34; -f $accessToken) \u0026#34;Accept\u0026#34; = \u0026#34;application/json\u0026#34; } # Get a list of repositories in the project $reposUrl = \u0026#34;$organizationUrl/$projectName/_apis/git/repositories?api-version=6.0\u0026#34; $reposResponse = Invoke-RestMethod -Uri $reposUrl -Headers $headers -Method Get Write-Host \u0026#34;Repos found \u0026#39;$reposResponse.count\u0026#39;\u0026#34; #create dictionary openBranchesPerUser $openBranchesPerUser = @{} foreach ($repo in $reposResponse.value) { $repoName = $repo.name if ($repo.isDisabled) { Write-Host \u0026#34;Skipping disabled repo: \u0026#39;$repoName\u0026#39;\u0026#34; -foregroundcolor Gray continue; } Write-Host \u0026#34;Checking \u0026#39;$repoName\u0026#39;:\u0026#34; -foregroundcolor yellow $branchesUrl = \u0026#34;$organizationUrl/$projectName/_apis/git/repositories/$repoName/refs?filter=heads\u0026amp;api-version=6.0\u0026#34; $branchesResponse = Invoke-RestMethod -Uri $branchesUrl -Headers $headers -Method Get foreach ($branch in $branchesResponse.value) { $branchName = $branch.name $BranchNameTrimmed = $branchName.replace(\u0026#39;refs/heads/\u0026#39;, \u0026#39;\u0026#39;) if ($BranchNameTrimmed -eq \u0026#39;master\u0026#39;) { continue; } if ($BranchNameTrimmed -eq \u0026#39;main\u0026#39;) { continue; } $encodedBranchName = [System.Uri]::EscapeDataString($branchName) $pushesUrl = \u0026#34;$organizationUrl/$projectName/_apis/git/repositories/$repoName/pushes?searchCriteria.includeRefUpdates\u0026amp;searchCriteria.refName=$encodedBranchName\u0026amp;api-version=6.0\u0026#34; ## Write-Host \u0026#34;$branchName - $pushesUrl\u0026#34; $pushesResponse = Invoke-RestMethod -Uri $pushesUrl -Headers $headers -Method Get # get first push in the list $push = $pushesResponse.value[0]; $firstPush = $pushesResponse.value[-1]; #Convert $lastPush.date to DateTime object $lastPushDate = [DateTime]::Parse($firstPush.date); # if the last push date is older than today minus the dayTolerance, skip the branch if ($lastPushDate -gt (Get-Date).AddDays(-$dayTolerance)) { Write-Host \u0026#34;Skipping \u0026#39;$repoName\u0026#39; - \u0026#39;$branchName\u0026#39; - last push date \u0026#39;$($lastPush.date)\u0026#39; compare date \u0026#39;$((Get-Date).AddDays(-$dayTolerance))\u0026#39; \u0026#39;$($push.pushedBy.uniqueName)\u0026#39;\u0026#34; -foregroundcolor Red continue; } else { Write-Host \u0026#34;Checking \u0026#39;$repoName\u0026#39; - \u0026#39;$branchName\u0026#39; - last push date \u0026#39;$($lastPush.date)\u0026#39; compare date \u0026#39;$((Get-Date).AddDays(-$dayTolerance))\u0026#39; \u0026#39;$($push.pushedBy.uniqueName)\u0026#39; \u0026#34; } $pushedBy = $firstPush.pushedBy.uniqueName # Add to openBranchesPerUser dictionary with the user name as key and a object as value including branch name, repositoy and respository url if ($openBranchesPerUser.ContainsKey($pushedBy)) { $openBranchesPerUser[$pushedBy] += [PSCustomObject]@{ Repository = $repoName RepositoryUrl = $repo.webUrl + \u0026#34;/branches?_a=all\u0026#34; Branch = $branchName firstPushDate = $firstPush.date lastPushDate = $push.date lastPusher = $push.pushedBy.uniqueName } } else { $openBranchesPerUser.Add($pushedBy, @([PSCustomObject]@{ Repository = $repoName RepositoryUrl = $repo.webUrl + \u0026#34;/branches?_a=all\u0026#34; Branch = $branchName firstPushDate = $firstPush.date lastPushDate = $push.date })) } } } # Write openBranchesPer user as a table, exclude RepositoryUrl $openBranchesPerUser.GetEnumerator() | ForEach-Object { Write-Host \u0026#34;User: $($_.Key)\u0026#34; -ForegroundColor Green $_.Value | Format-Table -Property Repository, Branch, firstPushDate, lastPushDate, lastPusher -AutoSize } # Write openBranchesPerUser to a JSON file $openBranchesPerUser | ConvertTo-Json | Out-File -FilePath \u0026#34;openBranchesPerUser.json\u0026#34; -Encoding ascii Output The output will be visible in the console and a JSON file.\n3. Get feedback Ask for feedback from your colleagues and manager. This feedback can be used to improve yourself in the future. It can also be used to improve the company you worked for. If you don\u0026rsquo;t ask for feedback, you will never know what you could have done better.\nWhen asking for feedback keep in mind it\u0026rsquo;s to improve yourself, not to get a compliment. You can ask for feedback in the following way:\n💬 \u0026ldquo;What could I have done better while working together?\u0026rdquo; 4. Say goodbye Take the time to say goodbye to your colleagues and express your gratitude for the time you spent working together. You never know when you might cross paths with them again in the future.\nMake sure you connect on social media with people you want to connect on the long term.\nChecklist :check_box_with_check: Hand over open work items, or unassign them :check_box_with_check: Delete open branches :check_box_with_check: Say Goodbye to your team and colleagues :check_box_with_check: Check for your ip whitelists in Azure DevOps Conclusion When leaving a company, you want to leave no technical debt behind and clean up after yourself. You can use the scripts in this article to help you with that.\nFurther reading A more complex system to generate emails created by Sven: https://github.com/sven73a/Powershell-Utils-Azure ","permalink":"https://martdegraaf.github.io/posts/consulting/job-done/","summary":"Leave no technical debt, clean up after yourself when leaving a company. The first impression is just as important as the last impression.","title":"Golden Tips To Leave Companies Without A Nasty Smell"},{"content":"We want to spend time on the most important things right? Are you also annoyed by the number of emails generated by Azure DevOps? See how I fixed the email overdose using an Outlook rule.\nE-mail headers To remove all emails would be nice, but we won\u0026rsquo;t see any mentions. To know how to recognize a mention email I opened a mention email. I found the headers by clicking the three dots in the Outlook web version. And then Open the \u0026lsquo;View\u0026rsquo; dropdown and select \u0026lsquo;View message details\u0026rsquo;.\nOutlook web - View \u0026gt; View message details We see that some headers indicate that the message trigger contained \u0026lsquo;Mention\u0026rsquo;. This is essential information for our next steps. Here below the headers I found in the email message.\n1 2 3 4 5 6 X-VSS-Scope: organisation/project/Repository X-VSS-Event-Type: ms.vss-mentions.identity-mention-event X-VSS-Subscription-ID: ms.vss-mentions.identity-mention-subscription X-VSS-Event-Initiator: Mart de Graaf X-VSS-Event-Initiator-Ascii: Mart de Graaf X-VSS-Event-Trigger: Mention Outlook Rules I am not praising the Outlook client, I want to talk about the Rules feature of Outlook. The rules are found in the classic Outlook by pressing \u0026lsquo;File\u0026rsquo; and then clicking the big \u0026lsquo;Manage Rules \u0026amp; Alerts\u0026rsquo; button. Let\u0026rsquo;s create a rule to delete and mark all those emails as read.\nThe first step is to define conditions. We know Azure DevOps emails from azuredevops@microsoft.com. For my case, I wanted to scope emails to a certain organization, which can be done by filtering specific words in the message header. Rule conditions The emails should be deleted, but also be marked as read. Nothing is more annoying than a number after your delete folder, right? Rule actions We still would like to receive the Mention emails. So we need to make an exception for them. As said earlier this can be done by filtering the message header for specific words. Rule exceptions Full Outlook Rule The only thing you have to edit in my example is the organisationName.\n1 2 3 4 5 from azuredevops@microsoft.com and with VSS-Scope: organisationName delete it and mark it as read except if the message header contains \u0026#39;X-VSS-Event-Trigger: Mention\u0026#39; or \u0026#39;X-VSS-Subscription-ID: ms.vss-mentions.identity-mention-subscription\u0026#39; :megaphone: Have any suggestions, I would love to hear them! Conclusion and discussion Help yourself keep control of your inbox. I think you should work from the board and business wishes, and not from your inbox.\n","permalink":"https://martdegraaf.github.io/posts/consulting/filter-devops-mentions-in-outlook/","summary":"Are you also annoyed by the number of emails generated by Azure DevOps? Learn how I fixed email overdose using an Outlook rule.","title":"Efficiently use Outlook Rules to Delete Azure DevOps Email Clutter"},{"content":"As a consultant, starting a new project with a client can be a daunting task. One way to make the transition smoother is by cloning all the repositories on your first day. This allows you to have quick access to all the necessary files and resources, enabling you to perform your job efficiently and effectively. In this blog post, we will explore the benefits of cloning repositories, a script for doing so, and some common pitfalls to avoid.\nSkip to the code sample\nOrganizing your Git repos When working for multiple clients or even just having private projects next to your client projects it can come in handy to organize your git repositories. For some Frontend repositories, the path with node_modules was too long and that forced me to place my folders on the Disk level. A path for a project for me would look like C:\\Git\\{ClientName}\\{RepositoryName}.\n1 2 3 4 5 6 7 8 9 10 11 C:\\Git ┣ Client1 ┃ ┣ Client1.Repository1 ┃ ┣ Client1.Repository2 ┃ ┗ Client1.Repository3 ┣ Client2 ┃ ┣ Client2.Repository1 ┃ ┗ Client2.Repository2 ┗ private ┃ ┣ Blog ┃ ┗ Demo Using workspaces in Git Fork Fork is a tool that will help you focus on the right workload. Using the structure as discussed with Fork, you can focus on the right repositories. Cloned new repositories but not seen by Fork? Reload the whole folder using right-click and \u0026lsquo;Rescan repositories\u0026rsquo;. Get Git Fork from git-fork.com.\nFork Repository Manager Use Fork Workspaces to focus on the current environment. It will also help you work on private projects outside of work hours on the same workstation. You can also create workspaces for different domains or teams if you are for example the lead or architect in a project.\nFork workspaces Configure your git username Depending on the network infra, you will need to configure your commit username to the email of your client. Some instances block all git pushes from committers with a different domain.\n1 git config [--global] user.email \u0026#34;username@corperate.com\u0026#34; In the script to clone all repositories, you can also enable the script to set the committer email for every repository.\nClone all repositories To clone all repositories in Azure DevOps we can use the REST API to find all existing repositories. The code example consists of a Powershell script and a configuration file with settings and Authorization.\nConfiguration Make sure to create a file named: CloneAllRepos.config with the contents written below. Make sure every parameter is configured as your workspace.\nCloneAllRepos.config 1 2 3 4 5 6 7 8 9 10 [General] Url=https://dev.azure.com/MART/project [LocalGitConfig] GitPath=C:\\Git\\ OrgName=MART [GitOptions] PruneLocalBranches=false # Optional defaults to false GitEmail=username@corperate.com \u0026#x1f4a1; Don\u0026rsquo;t know where to find a Personal Access Token in Azure DevOps? Read: Microsoft\u0026rsquo;s docs on personal access tokens.\nOn 2024 april 17, I updated the script to get an access token using the current session of the az cli. see Azure DevOps API Authentication.\nCloneAllRepos.ps1 When I first encountered the idea to clone all repos idea it was on a corporate wiki. After some backtracing, I found the source: Script to clone all Git repositories from your Azure DevOps collection.\nThe PowerShell script below does a git pull for existing repositories and performs a git clone on untracked repositories.\nI edited the script to fit my needs with some extra parameters.\nIt puts the repos in the given directory in settings. It prunes local branches when PruneLocalBranches is set to true. It sets the git username email to the configured GitUsername under GitOptions, it\u0026rsquo;s ignored when empty. CloneAllRepos.ps1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 # Read configuration file Get-Content \u0026#34;CloneAllRepos.config\u0026#34; | foreach-object -begin {$h=@{}} -process { $k = [regex]::split($_,\u0026#39;=\u0026#39;); if(($k[0].CompareTo(\u0026#34;\u0026#34;) -ne 0) -and ($k[0].StartsWith(\u0026#34;[\u0026#34;) -ne $True)) { $h.Add($k[0], $k[1]) } } #AzDO config $url = $h.Get_Item(\u0026#34;Url\u0026#34;) # LocalGitConfig $gitPath = $h.Get_Item(\u0026#34;GitPath\u0026#34;) $orgName = $h.Get_Item(\u0026#34;OrgName\u0026#34;) $pruneLocalBranches = $h.Get_Item(\u0026#34;PruneLocalBranches\u0026#34;) -eq \u0026#34;true\u0026#34; $gitEmail = $h.Get_Item(\u0026#34;GitEmail\u0026#34;) # Get the access token from current az login session # see https://learn.microsoft.com/en-us/azure/devops/integrate/get-started/authentication/service-principal-managed-identity?toc=%2Fazure%2Fdevops%2Forganizations%2Fsecurity%2Ftoc.json\u0026amp;view=azure-devops#q-can-i-use-a-service-principal-or-managed-identity-with-azure-cli $accessToken = az account get-access-token --resource 499b84ac-1321-427f-aa17-267ca6975798 --query \u0026#34;accessToken\u0026#34; --output tsv if ($null -eq $accessToken) { exit 1 } $headers = @{ \u0026#34;Authorization\u0026#34; = (\u0026#34;Bearer {0}\u0026#34; -f $accessToken) \u0026#34;Accept\u0026#34; = \u0026#34;application/json\u0026#34; } # Retrieve list of all repositories $resp = Invoke-WebRequest -Headers $headers -Uri (\u0026#34;{0}/_apis/git/repositories?api-version=1.0\u0026#34; -f $url) $json = convertFrom-JSON $resp.Content # Clone or pull all repositories $initpath = (\u0026#34;{0}{1}\u0026#34; -f $gitPath,$orgName) foreach ($entry in $json.value) { set-location $initpath $name = $entry.name Write-Host $name -ForegroundColor Green if($entry.isDisabled){ Write-Host \u0026#34;Skipping disabled repo: \u0026#39;$name\u0026#39;\u0026#34; -ForegroundColor Yellow continue; } $url = $entry.remoteUrl #-replace \u0026#34;://\u0026#34;, (\u0026#34;://{0}@\u0026#34; -f $gitcred) if(!(Test-Path -Path $name)) { git clone $url } else { Write-Host \u0026#34;Directory \u0026#39;$name\u0026#39; exists lets pull\u0026#34; set-location $name git pull $defaultBranch = git symbolic-ref --short HEAD if($pruneLocalBranches){ Write-Host \u0026#34;Pruning local branches $name\u0026#34; -ForegroundColor Yellow $branches = git branch -vv | Where-Object { $_ -notmatch \u0026#34;::\u0026#34; } | ForEach-Object { ($_ -split \u0026#39;\\s+\u0026#39;)[1] } foreach ($branch in $branches) { if ($branch -eq $defaultBranch) { Write-Host \u0026#34;Skipping default branch \u0026#39;$branch\u0026#39;.\u0026#34; continue } if ((git branch -vv | Where-Object { $_ -match \u0026#34;$branch\\s+\\[origin\\/\u0026#34; })) { Write-Host \u0026#34;Skipping branch \u0026#39;$branch\u0026#39; as it has a remote reference.\u0026#34; } else { git branch -D $branch Write-Host \u0026#34;Deleted local branch \u0026#39;$branch\u0026#39;.\u0026#34; -ForegroundColor Green } } } if($gitEmail){ git config user.email \u0026#34;$gitEmail\u0026#34; } } } \u0026#x1f916; If you have some additional ideas, let ChatGPT help you. Supply ChatGPt with the context: Rewrite this PowerShell script to also \u0026lt;insert new Feature\u0026gt;. Here is the current version of the PowerShell script: \u0026lt;insert PowerShell script\u0026gt;.. Let me know if you thought of a clever solution. Run it Run the script it using a PowerShell prompt for example using for example Windows Terminal.\n1 ./CloneAllRepos.ps1 Using scripting for common tasks In the world of microservices, we choose to duplicate some of the plumbing. When you want to change multiple repos knowledge on scripting can be helpful. In this series, I explored how to automate git tasks with PowerShell.\nSome examples are:\nUpdating multiple NuGet packages. Enforcing certain Nuget. config configurations. Renaming business terminology on multiple branches. Automating With this structure, you could automate actions over multiple repositories. In the code below I wrote an example of automating script for changing the Nuget.config file in every repository. If your packages have the same layout changes can be done easier and faster. Also, please check out my article using binary replace.\n1 2 3 4 5 6 7 8 9 10 11 12 git checkout main git pull git checkout -b fix/nugetconfig # DO THE NECESSARY CHANGE in nuget.config. git mv -f NuGet.config nuget.config git add * git commit -m \u0026#34;Only use private Nuget upstream\u0026#34; git push --set-upstream origin fix/nugetconfig git checkout main Conclusion and discussion Make your workflow faster with scripting and your knowledge of the Git CLI. When you have to do repetitive tasks such as updating a single package on multiple (microservice-like) repositories, try to automate it. It may for the first occurrence not be profitable, but after three times, you will be faster than doing it manually. It can also help you clean up your workspace and be more tidy.\n","permalink":"https://martdegraaf.github.io/posts/consulting/git-clone-all-repos-azure-devops/","summary":"Learn how to efficiently clone all Git repos in Azure DevOps with our comprehensive consulting guide. Streamline your development workflow today!","title":"Efficiently Git Clone All Repositories from Azure DevOps using PowerShell: A Step-by-Step Guide"},{"content":"So a few years ago I worked with Jaap Lamfers. Jaap showed me a tool to easily replace .csproject names and namespaces. This tool is named Bire.\nWhy you should care Renaming files is hard. When renaming Projects in Visual Studio the Folder does not change and keeps its original name. The project path is declared in the Solution file (.sln). Not a problem when working in Visual Studio, but a minor annoyance when reviewing a pull request.\nAlso when renaming projects via Visual Studio, not all namespaces will change as well. And when renaming manually you need to change the references as well in the contents of the .csproject file.\nBire Jaap encountered a problem where with microservices he needed to build and the namespaces could be changed afterward. The company used abbreviations for services, with bire he just could build and change the name afterward with a single command.\nThe tool of Jaap is available at Github jlamfers/bire.\nI thought his tool was brilliant, but it missed the crucial documentation to make it understandable for the whole company.\n\u0026#x1f37a; Bire is pronounced as the Dutch word \u0026lsquo;Bier\u0026rsquo;. Bier is Dutch for beer. BiRe is an abbreviation for \u0026lsquo;binary replace\u0026rsquo;. Using Bire bire -from C:\\git\\source -to C:\\git\\target -replace this=that Something=Anything\nThis is an example I wrote in the Readme file. But I want to give you a more concrete example let\u0026rsquo;s imagine this folder structure. The project is in C:\\git\\MyCoolProject\\source.\n1 2 3 4 5 6 7 8 9 10 11 12 source ┣ MyCoolProject.DataAccess ┃ ┣ .... ┣ MyCoolProject.DataModels ┃ ┣ .... ┣ MyCoolProject.Business ┃ ┣ .... ┣ MyCoolProject.WebApi ┃ ┣ .... ┣ MyCoolProject.Functions ┃ ┣ .... ┣ MyCoolProject.sln Let\u0026rsquo;s say we don\u0026rsquo;t like MyCoolProject anymore and we want to rename it to MCP.\nThen we can use Bire. Download the bire.exe from my GitHub release 1.\nUse the following command to get MyCoolProject abbreviated to MCP.\n1 bire -from C:\\git\\MyCoolProject\\source -replace MyCoolProject=MCP We\u0026rsquo;re not copying the project we don\u0026rsquo;t need the -to param. You should have your repo in git to do this change safely right?\nIgnoring the right files and file types What if we would have certain files that would refer to MyCoolProject and we want them to stay that way instead of renaming MyCoolProject to MCP. For example a MyCoolProjectClient.cs class.\n1 2 bire -from C:\\git\\MyCoolProject\\source -replace MyCoolProject=MCP -ignore \u0026#34;MyCoolProjectClient.cs|(.*(\\.|\\/|\\\\)(exe|dll|obj|bin|pdb|zip|\\.git|\\.vs|cache|packages))$\u0026#34; Conclusion Bire helped me a lot at some companies. Let tools help you and get grip on your time back. Let it be another tool in your toolbox. Let me know if you have used bire and liked it.\n","permalink":"https://martdegraaf.github.io/posts/replacing-your-projects-and-namespaces-using-bire/","summary":"We all sometimes want to rename projects and or namespaces but VS makes it hard, why can\u0026rsquo;t we just take one command to replace all? We can with bire.","title":"Replacing Your Projects and Namespaces Using Bire"},{"content":"TL;DR Just start your creating your first page if you want to blog. Come back if you need motivation, then go back to writing/typing.\nBlog in an hour My coworker said to me; \u0026ldquo;In the past time there was someone who did the things I dreamed of in just 4 days.\u0026rdquo;. The person he referred to was me. He praised me for my dedication and courage to go fast and agile. I went fast to production with my blog and iterated new articles and new features. My coworker planned things out like the theme. When he was not fully satisfied with his theme he chose another theme but still had no active blog post. This blog is written to help others to get started. So beat your excuses and start today.\nFocus on your first post To break the chicken-egg problem, the focus for starting your blog should be creating the first post. If there is no content why should you make a blog? If there is no blog why should you make content? Break through the cycle and start creating your first article.\nEat that frog \u0026lsquo;Eat that frog\u0026rsquo; states that you should prioritize your tasks, and do the thing with the most impact first. You should be acting like there is no tomorrow and start working on your blog today, except when you don\u0026rsquo;t want a blog. If you do not want a blog then why are you reading this article, are you procrastinating other tasks?\nSee your first article or blog post as the first checkpoint of your blogging world. From there you can check off multiple other checkpoints.\nTo read more about why you should do this now and not later, read my Book review on Eat that frog.\nOne-time setup I recommend using a blog builder that uses git versioning like Hugo. Hugo can be easily deployed. This blog runs on Azure Static Webapps and GitHub Pages. Other alternatives like Wix or WordPress will be more painful to maintain.\nJust because this blog iterated to Azure Static Webapps, didn\u0026rsquo;t stop me from producing the actual blog content. The blog content is the most valuable. It\u0026rsquo;s the core reason for your blog to exist. People are coming to read what you have written not for your cool styling.\nTo make it agile I have made these acceptance criteria for your blog.\nYour blog has to be accessible through the browser. Your blog has at least one article. That means that these things are OUT OF SCOPE for your first production release:\nA custom URL A funky-looking theme A fitting Favicon An about page. Comment section And so much more that gets in your way for your first blog post to be online. Iterating your blog Iterating your blog should be about:\nFinetuning your blogging workflow. Styling your blog. Refactoring your content. Learning about the blogging framework. Be fearless, when using Hugo and git you have version control to go back to. Just don\u0026rsquo;t commit and push any secrets. Prioritize your blog TODO list Make a list of items you want to blog about. When you get new ideas put them on the list and prioritize them. Start with your first item and focus on it until you finish that blog post!\nSome questions to help you prioritize:\nWhat blog item/subject has the most impact on your career? For example, if you write about a subject you work on daily, you will learn and become better. Another example is: you want to help companies using a certain technology. Writing about a subject helps your branding and the teacher learns the most. What blog item/subject is hardest to solve? What blog item/subject is not yet available easily when you searched for your problem? What blog item/subject gives you the most joy? What story do you tell about what you do for a living? What do colleagues admire about you? Use a mindmap to organize your ideas The mermaid JavaScript plugin builds some cool diagrams. Get started with your mindmap for blogging subjects right now with the Mermaid Live Editor. Below is an example mindmap of my blogging subjects.\nmindmap root)Blog mindmap( T[Testing] Playwright Builder pattern UI scraping LaC((Life as\nconsultant)) C[Clone and go] Clone script Git folder structure Personal development planinng Client conventions Clear all traces A[Azure] Application insights BA[Blogging Agile] Getting started Template Conclusion When starting your blog, focus on your content first. That\u0026rsquo;s the thing that will be truly yours. Keep an eye out for more content about blogging agile.\n","permalink":"https://martdegraaf.github.io/posts/blogging-agile/start-blogging-today/","summary":"How to start with your blog. The main focus should be content, starting and moving forward.","title":"Beat your excuses and start blogging"},{"content":"This year I read and listened to \u0026lsquo;Eat that frog!\u0026rsquo; by Brian Tracy. I tried to apply these principles to my daily life.\n\u0026#x1f4a1; This article will be updated when I find new takeaways. Summary Eat that frog is a practical book on how to \u0026ldquo;get more of the important things done today\u0026rdquo;. It focuses on doing key tasks first and procrastinating the unimportant tasks.\nThe author gives 21 principles for getting more things done faster:\nSet the table (spend some time writing out your goals for year, month, and week). Prioritize those and concentrate on the most important ones. Plan every day in advance (on paper). Apply the 80/20 rule to everything: 20% of your work will constitute 80% of your achievements. Concentrate on that 20%. Consider the consequences of what you choose to work on. Focus on these above all else. Practice creative procrastination. You\u0026rsquo;re not going to be able to get everything done so put off the less important tasks. Use the ABCDE method. Organize your tasks by value (a,b,c\u0026hellip;) and priority (1,2,3\u0026hellip;). Focus on key result areas. Obey the law of three. Identify the three things you do in work that account for 90% of your contribution. Prepare thoroughly before you begin. The 6Ps: Proper Prior Planning Prevents Poor Performance. Take it one barrel at a time. That way you can make it through the desert. Upgrade your key skills! Identify your key constraints. Put pressure on yourself. Motivate yourself into action (be your own cheerleader). technology is a terrible master. Take back time your time from tech addictions. Learn to turn off devices and notifications and keep them off. Technology is a wonderful servant. Use technological tools to confront yourself with what is most important. Focus your attention Slice and dice your tasks. Break your tasks into smaller, more manageable chunks. Create large blocks of time to get your work done. Develop a sense of urgency. Single-handle every task. Comparing Agile and the lecture in Eat that frog I see many aspects that are similar in working Agile as what is stated in \u0026lsquo;Eat that frog\u0026rsquo;. For example:\n(6) Use the ABCDE Method Continually (The Product owner should keep the backlog up to date) (7) Focus on key result areas (The work in the sprint) (9) Prepare thoroughly before you begin (refinements) (18) Slice and Dice the Tasks (The division of Epic Feature and Product backlog, and task level) (20) Develop a sense of urgency (The sprint is time-boxed) Lessons learned as a developer Think about your daily, weekly, monthly, and long-term goals. Make sure you check your daily goals with your long-term goals. I did write down my long-term goals on paper and am trying to work with daily TODO lists.\n\u0026#x1f4ac; \u0026ldquo;Find out what successful people do, and do the same things until you get results.\u0026rdquo;\nQuote from the book\nExample long-term goals I am certified in security in 2023. I am speaking on stage as a public speaker in 2023. I am 90kg or less in November 2023. Important is to write your goals in the present tense, positive voice, and in the first person singular. This way your mind will accept it as a clear goal.\nFor each of these goals, create a list of work you have to do to achieve your goal. When you break down your goal into tasks it will be easier to do.\nI did this for example for my speaking goals. The Gantt chart below was my plan for 2022. The main goal was to be prepared for DevNetNoord, a Dutch tech event. When I determined my goal I placed tasks that could help me prepare for this event, such as training or smaller sessions. To be transparent to my client I included the sprints, that way I can easily communicate when I am busy.\ngantt title Marts Speaking path dateFormat YYYY-MM-DD section Private Holidays :v1, 2022-08-08, 3w section Client sprints Sprint 14 :s14, 2022-07-11, 2w Sprint 15 :s15, after s14, 2w Sprint 16 :s16, after s15, 2w Sprint 17 :s17, after s16, 2w Sprint 18 :s18, after s17, 2w Sprint 19 :s19, after s18, 2w Sprint 20 :s20, after s19, 2w section Other consultancy days Consultancy client 1 : 2022-07-27, 2d Consultancy client 2 : 2022-08-02, 1d section Speaking Training Speaking in public (training) 1 :a1, 2022-09-16, 1d Speaking in public (training) 2 :a1, 2022-09-23, 1d section Events Pizza session 4dotnet Application insights : milestone, m1, 2022-06-02, 2min Pizza session 4dotnet Advanced Git : milestone, m1, 2022-09-02, 2min DevNetNoord : milestone, m1, 2022-09-29, 2min DotNetFriday : milestone, m1, 2022-11-04, 2min Multiple TODO lists Imagine you are in a team using a kanban board, but besides that, you have an on-paper TODO list. That does seem like you are creating a black market of work on paper next to your board of work, right? Some work may be investigating issues to refine for example an upcoming feature. How do you keep your board work and on-paper lists in sync?\nUse procrastination as your friend Some things you rate as unimportant can be procrastinated on. Try to procrastinate the unimportant tasks in your daily work to get the important work and tasks done.\nRecommendations I recommend reading or listening to this book and practicing some of the given habits. One that will highly depend on the individual is: \u0026lsquo;Upgrade your key skills\u0026rsquo;. I think you should be making goals to keep learning next to your daily work. Sometimes even before doing any daily work. The learning will increase your efficiency especially when you focus on a key skill.\nRead Eat that frog Buy the book via Amazon or listen via Spotify:\n","permalink":"https://martdegraaf.github.io/reviews/books/eat-that-frog/","summary":"\u003cp\u003eThis year I read and listened to \u0026lsquo;Eat that frog!\u0026rsquo; by Brian Tracy. I tried to apply these principles to my daily life.\u003c/p\u003e\n\u003cdiv style=\"\nborder: #0078D3 1px solid;\npadding: 15px;\nbackground: var(--entry);\nborder-left-width: 3px;\"\u003e\n   \u0026#x1f4a1; This article will be updated when I find new takeaways.\n\u003c/div\u003e\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003eEat that frog is a practical book on how to \u0026ldquo;get more of the important things done today\u0026rdquo;. It focuses on doing key tasks first and procrastinating the unimportant tasks.\u003c/p\u003e","title":"Eat that frog! - Book review by a developer"},{"content":"TL;DR The Application Insights was configured to send the data twice to the Log Analytics workspace. One of them was done by ARM, the other was enforced by an Azure Policy. We turned off the Azure Policy and removed all upstream diagnostics settings on all Application Insights resources. This saved 100 euros monthly.\nOn a recent project, we encountered duplicate logging in Azure Application insights.\n\u0026#x1f4a1; In the Classic Azure world, Log workspaces were not a thing yet. So when creating new application insights resources you should not encounter this problem. Problem introduction scope, and context As seen in the screenshot we suffered in the acceptance environment with duplicate exceptions, information, and dependencies. In the development environment, on the left screen, we did not experience this issue. \u0026#x1f60e; Sensitive information in the screenshots is blurred for obvious reasons. Exclude the software error To exclude the possibility of a software error, we executed these actions:\nDebugging the application and looking at the outgoing application insights tab. The Azure web app / Azure function is misconfigured. We tested web apps with a single instance. If a single instance generates duplicate logging, it\u0026rsquo;s surely not the instance count. The cause The Log Analytics Workspace was configured in diagnostic settings as well it was in the properties of the Application Insights. See the screenshots of the Azure portal. The screenshot below shows how the application insights resource is configured upstream. So when we concluded the configuration was duplicate, we asked ourselves:\nWhy was it duplicate configured, couldn\u0026rsquo;t that have been spotted in an existing ARM template? The answer consists of multiple factors and solutions. The two main components were an ARM template and an Azure Policy. 1. The correct way - ARM \u0026gt; Workspace property The correct implementation of the properties was created by an ARM template. The infrastructure as code makes sure we\u0026rsquo;re configuring the upstream Log Analytics Workspace.\n1 2 3 4 5 6 7 8 9 10 11 12 { \u0026#34;type\u0026#34;: \u0026#34;microsoft.insights/components\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;other\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ai-[YOUR-APPLICATION-INSIGHTS-NAME]\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2020-02-02-preview\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;West Europe\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;Application_Type\u0026#34;: \u0026#34;web\u0026#34;, \u0026#34;ApplicationId\u0026#34;: \u0026#34;ai-[YOUR-APPLICATION-INSIGHTS-NAME]\u0026#34;, \u0026#34;WorkspaceResourceId\u0026#34;: \u0026#34;law-[YOUR-LOG-ANALYTICS-WORKSPACE-NAME]\u0026#34; } } \u0026#x1f4a1; The naming of Azure resources is done using the Azure abbreviations guide. 2. Azure Policy was enforced on \u0026lsquo;Diagnostic settings\u0026rsquo; There also was an Azure policy checking that there was a diagnostic setting for sending data to the Log Analytics Workspace. Whenever the IT operations checked and enforced the Azure Policy, we would have duplicated the upstream to our Log Analytics Workspace.\nWe turned off this Azure Policy and removed all upstream diagnostics settings on all Application Insights resources.\nConclusion Difference Application Insights and Log Analytics workspace graph TD AI1[Application Insights instance 1] AI2[Application Insights instance 2] LA1[Log Analytics Workspace] LA1 \u003c--- AI1 LA1 \u003c--- AI2 Application Insights gives \u0026lsquo;insights\u0026rsquo; into application logging, exceptions, and such. You can use the Kusto query language to fetch data intelligently from Application Insights. The Log Analytics workspace is a set of tables. For the client in this article, the data of the Application insights was forwarded to the Log Analytics workspace. The advantage of the Log Analytics workspace is to query over multiple Application insights as well as data about other resources in Azure, such as API management, application gateways, service busses, or firewalls.\nIn the screenshot below is seen that when you create a new Application Insights resource the Log Analytics Workspace is configured automatically. \u0026#x2139;\u0026#xfe0f; The Log Analytics workspace is part of the Azure Monitor component in Azure. Cost analysis \u0026#x1f4b0; This change saved the client over €1000 monthly in Azure Log Analytic costs. The euros won\u0026rsquo;t tell you how much of the total it was. After fixing all duplicate configurations, the total costs of the Application Insights resources were reduced by roughly 40-50%. If you also are experiencing this problem, I hope this article helps. Good logging makes all developers happy.\nWrap up Whenever you see duplicate logging in your application insights make sure the configuration is correct. Also, make sure that you\u0026rsquo;re not forcing a policy on the diagnostic settings when you configure it in the properties. Only one upstream to the Log Analytic workspace is required \u0026#x1f609;.\nReferences Microsoft Learn - Application Insights Duplicate Telemetry Converting table ApplicationInsights LogAnalytics Azure Monitor ","permalink":"https://martdegraaf.github.io/posts/duplicate-logging-azure-application-insights/","summary":"Save troubles and money fixing duplicate logging in your Azure Application Insights, and Log Analytics Workspace.","title":"Duplicate Logging Azure Application Insights"},{"content":"Introduction For a recent project, I wanted to add a property to the consuming applications from within my NuGet package. This prevents making a pull request for every consuming application with a .csproj change.\n\u0026#x1f4d3; Please note in this example the NuGet package has the ID: MyProject.ExampleNuGet, so replace that value for your nuget package. The consuming application is MyProject.ConsumingWebApi. NuGet file structure 1 2 3 4 5 6 7 8 MyProject.ExampleNuGet (Repository level) ┣ MyProject.ExampleNuGet ┃ ┣ Extensions ┃ ┃ ┗ MySpecialThing.cs ┃ ┣ MyProject.ExampleNuGet.csproj ┃ ┗ MyProject.ExampleNuGet.props ┣ MyProject.ExampleNuGet.sln ┗ nuget.config MyProject.ExampleNuGet.props The NuGet package has a .props-file to enforce some properties to the consumers.\n1 2 3 4 5 6 7 8 \u0026lt;Project\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;!-- Enable output XML Docs for Swagger. --\u0026gt; \u0026lt;GenerateDocumentationFile\u0026gt;true\u0026lt;/GenerateDocumentationFile\u0026gt; \u0026lt;!-- Disable missing XML comment warnings. --\u0026gt; \u0026lt;NoWarn\u0026gt;$(NoWarn);1591\u0026lt;/NoWarn\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;/Project\u0026gt; MyProject.ExampleNuGet.csproj Important is to set the build action of the MyProject.ExampleNuGet.props file to package it to the build directory. See the example below:\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026lt;Project Sdk=\u0026#34;Microsoft.NET.Sdk\u0026#34;\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;TargetFramework\u0026gt;net6.0\u0026lt;/TargetFramework\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;None Update=\u0026#34;MyProject.ExampleNuGet.props\u0026#34; Pack=\u0026#34;true\u0026#34; PackagePath=\u0026#34;build\u0026#34;\u0026gt; \u0026lt;/None\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;/Project\u0026gt; Conclusion When installing this package on for example MyProject.ConsumingWebApi a file is generated in de build folder MyProject.ConsumingWebApi.csproj.nuget.g.targets. This ensures the setting is on when building MyProject.ConsumingWebApi.\nReferences I used the following resources to fix my problem.\nhttps://stackoverflow.com/questions/67263924/create-nuget-containing-shared-project-properties-automatic-references https://learn.microsoft.com/en-us/nuget/concepts/msbuild-props-and-targets#packagereference-projects ","permalink":"https://martdegraaf.github.io/posts/add-properties-for-consuming-apps-to-a-nuget-package/","summary":"This article explains how to add consuming project properties to a NuGet package. Those project properties will be used in the consuming apps.","title":"Add project properties for consuming apps to a NuGet package"},{"content":"Introduction For a recent project, I wanted to create a test that verified that LogError was called. Consider for example this piece of code below. The catch operation was added to swallow the exception of the delete action. We want to test this behavior but still would like to know if the LogError is being called.\nThe system under test 1 2 3 4 5 6 7 8 9 10 11 12 13 14 public async Task Delete(long sequenceNumber) { _logger.LogInformation(\u0026#34;Deleting `{sequenceNumber}`.\u0026#34;, sequenceNumber); try { await _client.Delete(..); _logger.LogInformation(\u0026#34;Delete completed `{sequenceNumber}`.\u0026#34;, sequenceNumber); } catch (InvalidOperationException ex) when (ex.Message.Equals($\u0026#34;The scheduled message with SequenceNumber = {sequenceNumber} is already being cancelled.\u0026#34;)) { _logger.LogError(ex, \u0026#34;Already cancelled {sequenceNumber}.\u0026#34;, sequenceNumber); } } Verify that LogError is called Have you ever tried to verify your LogError using xUnit? It does not seem to work out of the box as other FakeItEasy. I tried this code for example, but it just would not work. The mock that throws the exception has been left out to keep the code sample small.\n1 2 3 4 5 6 7 8 9 10 //Arrange var logger = A.Fake\u0026lt;ILogger\u0026lt;SystemUnderTest\u0026gt;\u0026gt;(); var sut = new SystemUnderTest(logger); //Act await sut.Delete(1); //Assert A.CallTo(() =\u0026gt; logger.LogError(A\u0026lt;string\u0026gt;.Ignored, A\u0026lt;object[]\u0026gt;.Ignored)) .MustHaveHappenedOnceExactly(); The LoggerExtensions class The solution was right at hand because my coworker had already figured it out. Thanks, Marnix. Chekout his blog: Marnix\u0026rsquo; blog. Use the extension class as described below.\n1 2 3 4 5 6 7 8 9 10 //Arrange var logger = A.Fake\u0026lt;ILogger\u0026lt;SystemUnderTest\u0026gt;\u0026gt;(); var sut = new SystemUnderTest(logger); //Act await sut.Delete(1); //Assert logger.VerifyLogged(LogLevel.Information, \u0026#34;Deleting 1\u0026#34;); logger.VerifyLogged(LogLevel.Error, \u0026#34;Already cancelled 1\u0026#34;); LoggerExtensions.cs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 using FakeItEasy; using Microsoft.Extensions.Logging; using System; using System.Linq; using Xunit.Sdk; namespace SomeCoolNamespace; public static class LoggerExtensions { public static void VerifyLogged\u0026lt;T\u0026gt;(this ILogger\u0026lt;T\u0026gt; logger, LogLevel level, string logMessage) { var (found, actualLevel, actualMessage) = logger.VerifyLog(logMessage); if (!found) { throw new XunitException($\u0026#34;No log message found containing \u0026#39;{logMessage}\u0026#39; at any loglevel\u0026#34;); } if (actualLevel != level) { throw new AssertActualExpectedException( $\u0026#34;[{level}] {logMessage}\u0026#34;, $\u0026#34;[{actualLevel}] {actualMessage}\u0026#34;, $\u0026#34;Unexpected log level for log message\u0026#34;); } } public static void VerifyNotLogged\u0026lt;T\u0026gt;(this ILogger\u0026lt;T\u0026gt; logger, LogLevel level, string logMessage) { var (found, actualLevel, actualMessage) = logger.VerifyLog(logMessage); if (found \u0026amp;\u0026amp; actualLevel == level) { throw new XunitException( @$\u0026#34;Log message found containing \u0026#39;{logMessage}\u0026#39; at level {level}{Environment.NewLine}Message: {actualMessage}\u0026#34;); } } public static void VerifyNotLoggedAtLevel\u0026lt;T\u0026gt;(this ILogger\u0026lt;T\u0026gt; logger, LogLevel level) { var call = Fake.GetCalls(logger) .FirstOrDefault(call =\u0026gt; (LogLevel?)call.Arguments[0] == level); if (call != null) { throw new XunitException( @$\u0026#34;Log message found at level {level}{Environment.NewLine} Message: {call.Arguments[2]}\u0026#34;); } } public static void VerifyLoggedAtLevel\u0026lt;T\u0026gt;(this ILogger\u0026lt;T\u0026gt; logger, LogLevel level) { var found = Fake.GetCalls(logger) .Any(call =\u0026gt; (LogLevel?)call.Arguments[0] == level); if (found) { throw new XunitException($\u0026#34;No log message found at level {level}\u0026#34;); } } private static (bool found, LogLevel? level, string? message) VerifyLog\u0026lt;T\u0026gt;(this ILogger\u0026lt;T\u0026gt; logger, string message) { var call = Fake.GetCalls(logger) .FirstOrDefault(call =\u0026gt; call.Arguments[2].ToString() .Contains(message, StringComparison.OrdinalIgnoreCase)); return (call != null, (LogLevel?)call?.Arguments[0], call?.Arguments[2].ToString()); } } Conclusion Using this class you will be able to test your logging with Xunit and FakeItEasy.\n","permalink":"https://martdegraaf.github.io/posts/verifylogged-with-xunit/","summary":"Explains how to test LogError with Xunit.","title":"How to verify that ILogger logged an error?"},{"content":" Mart de Graaf is a hardworking software geek sharing his ideas, solutions, and hobbies. He is enthusiastic about .NET solutions, Azure DevOps, and Azure solutions. Mart is working for 4Dotnet, a consulting company in the Netherlands. At home, he is a father to his four children.\nCore values If you ask some of my previous coworkers, what defines Mart? They will tell you that I\u0026rsquo;m a determined and hardworking person. I will prove in my blog that it\u0026rsquo;s true. One of the things I wanted for this blog is to show in my blog are Mermaid diagrams. I used for example a Gantt chart and a mindmap to share my ideas. Press the moon or sun icon in the menu to switch from light to dark or vice versa and see that the mermaid theme will also change. Pro tip: use Alt + T to switch themes. But as you can see I have not yet fixed the certification part as the title has the same color as the background.\nSpeaking Sharing makes me so happy that I try to share my knowledge through speaking. See how I planned for that using some lessons I learned from the book: Eat that frog.\nCertifications Here are some of my active certifications I am very proud of:\nMicrosoft\u0026rsquo;s certifications can be found at Transcript - Mart de Graaf Microsoft Learn\n","permalink":"https://martdegraaf.github.io/about/","summary":"\u003cstyle\u003e\n   .profile-box{\n      background-color: var(--entry);\n      border-radius: var(--radius);\n      display: flex;\n      flex-direction: row-reverse;\n      align-content: stretch;\n   }\n   .profile-image{\n      border-radius: 100%!important;\n      margin:1rem!important;\n      border: 3px solid #0078d4;\n   }\n   .profile-content{\n      vertical-align: middle;\n      justify-content: center;\n      align-content: center;\n      flex-direction: column;\n      display: flex;\n      padding: 1.3rem;\n      font-size: 1.3rem;\n   }\n@media (max-width: 800px) {\n   .profile-box{\n      flex-direction: column;\n   }\n  }\n\u003c/style\u003e\n\u003cdiv class=\"profile-box\"\u003e\n   \n   \n   \u003cimg class=\"profile-image\" src=\"/images/mart_square_hu11878351633383606924.jpg\" width=\"200\" height=\"200\"\u003e\n   \u003cdiv class=\"profile-content\"\u003e\n      \u003cp\u003eMart de Graaf is a hardworking software geek sharing his ideas, solutions, and hobbies. He is enthusiastic about .NET solutions, Azure DevOps, and Azure solutions. Mart is working for \u003ca href=\"https://www.4dotnet.nl/\" target=\"_blank\" rel=\"noopener\"\u003e4Dotnet\u003c/a\u003e, a consulting company in the Netherlands. At home, he is a father to his four children.\u003c/p\u003e","title":"About Mart"},{"content":"I am a seasoned speaker passionate about sharing knowledge with the tech community. I\u0026rsquo;ve spoken at various events and specialize in topics like Git, cloud software observability, and the optimization of daily work for software engineers. If you are interested in having me as a speaker at your event, please reach out!\nCurrent sessions For bookings, feel free to contact me directly or via Sessionize. Here are the sessions that I can deliver on short notice:\nAdvanced git and git workflows - A deep dive into advanced Git concepts and workflows to streamline your version control processes. Guide to Application Insights, Lag Analytics and KQL - Learn how to monitor, debug, and control your cloud-based software with Microsoft\u0026rsquo;s Application Insights. Observability of your cloud Software - Application Insights deep dive - Learn how to monitor, debug, and control your cloud-based software with Microsoft\u0026rsquo;s Application Insights. Improve your daily work using habits for Software Engineers (New) - Boost your productivity by integrating these proven habits into your daily routine as a software engineer. Previous engagements Here are some of the events where I had the privilege to share my knowledge:\nDevCampNoord 16 November 2023 - 4Dotnet\u0026rsquo;s Move up 26 October 2023 - DevNetNoord 7 September 2023 - Azure Platform Engineering meetup 4 April 2023 - DevCampNoord. 4 November 2022 - DotNetFriday (Recording) 29 September 2022 - DevNetNoord (The Revival) 17 October 2019 - NoorderTest 29 November 2018 - Young Sogetist of the Year 2018 Finale - Recording available upon request Looking forward to engaging with you at the next tech event!\n","permalink":"https://martdegraaf.github.io/speaking/","summary":"\u003cp\u003eI am a seasoned speaker passionate about sharing knowledge with the tech community. I\u0026rsquo;ve spoken at various events and specialize in topics like Git, cloud software observability, and the optimization of daily work for software engineers. If you are interested in having me as a speaker at your event, please reach out!\u003c/p\u003e\n\u003c!-- \n## Upcoming engagements --\u003e\n\u003ch2 id=\"current-sessions\"\u003eCurrent sessions\u003c/h2\u003e\n\u003cp\u003eFor bookings, feel free to contact me directly or via \u003ca href=\"https://sessionize.com/mart-de-graaf/\" target=\"_blank\" rel=\"noopener\"\u003eSessionize\u003c/a\u003e. Here are the sessions that I can deliver on short notice:\u003c/p\u003e","title":"Speaking engagements"}]